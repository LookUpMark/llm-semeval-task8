{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "56baa9b3",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Submission Pipeline\n",
                "\n",
                "This notebook generates valid submission files for **Task A**, **Task B**, and **Task C** in a **single run**.\n",
                "\n",
                "**Workflow**:\n",
                "1.  **Load Data**: Reads the test dataset.\n",
                "2.  **Run Pipeline**: Executes your RAG system ONCE per question.\n",
                "3.  **Format Outputs**: Creates three separate lists of results (one per task).\n",
                "4.  **Save Files**: Writes `submission_TaskA...`, `submission_TaskB...`, `submission_TaskC...`.\n",
                "5.  **Validate**: Checks all files against requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5d409de5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import sys\n",
                "from typing import List, Dict, Any\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Add project root to path\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "# Import RAG pipeline\n",
                "try:\n",
                "    from src.graph import app\n",
                "except ImportError:\n",
                "    print(\"ERROR: Cannot import 'src.graph.app'. Run from 'eval/' directory.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d409bffa",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70c8b770",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "\n",
                "INPUT_FILE = \"../dataset/human/generation_tasks/reference.jsonl\"\n",
                "OUTPUT_DIR = \"../data/submissions\"\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Team: {TEAM_NAME}\\nInput: {INPUT_FILE}\\nOutput Dir: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a91fc589",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
                "    data = []\n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                if line.strip():\n",
                "                    data.append(json.loads(line))\n",
                "        print(f\"Loaded {len(data)} items.\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"ERROR: File {file_path} not found.\")\n",
                "    return data\n",
                "\n",
                "test_data = load_jsonl(INPUT_FILE)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "50ef8c3b",
            "metadata": {},
            "source": [
                "## 2. Execution Loop (Single Pass)\n",
                "We run the inference once and structure the data for all 3 tasks:\n",
                "*   **Task A**: `contexts` only.\n",
                "*   **Task B**: `contexts` + `predictions`.\n",
                "*   **Task C**: `contexts` + `predictions`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a4936b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "subs_A = []\n",
                "subs_B = []\n",
                "subs_C = []\n",
                "\n",
                "print(\"Running pipeline...\")\n",
                "\n",
                "for item in tqdm(test_data):\n",
                "    question = item.get(\"question\")\n",
                "    \n",
                "    # --- 1. Run Pipeline ---\n",
                "    try:\n",
                "        result = app.invoke({\"question\": question})\n",
                "    except Exception as e:\n",
                "        print(f\"ERROR processing '{question}': {e}\")\n",
                "        result = {}\n",
                "    \n",
                "    # --- 2. Format Contexts ---\n",
                "    raw_docs = result.get(\"documents\", [])\n",
                "    formatted_contexts = []\n",
                "    for doc in raw_docs:\n",
                "        meta = getattr(doc, \"metadata\", {})\n",
                "        content = getattr(doc, \"page_content\", \"\")\n",
                "        doc_id = meta.get(\"id\", meta.get(\"document_id\", \"unknown\"))\n",
                "        score = meta.get(\"relevance_score\", meta.get(\"score\", 0.0))\n",
                "        \n",
                "        formatted_contexts.append({\n",
                "            \"document_id\": doc_id,\n",
                "            \"text\": content,\n",
                "            \"score\": float(score)\n",
                "        })\n",
                "        \n",
                "    # --- 3. Format Predictions ---\n",
                "    gen_text = result.get(\"generation\", \"\")\n",
                "    if not gen_text: gen_text = \"No answer generated.\"\n",
                "    formatted_predictions = [{\"text\": gen_text}]\n",
                "\n",
                "    # --- 4. Build Task Items ---\n",
                "    \n",
                "    # Task A: Contexts only\n",
                "    item_A = item.copy()\n",
                "    item_A[\"contexts\"] = formatted_contexts\n",
                "    subs_A.append(item_A)\n",
                "    \n",
                "    # Task B: Contexts + Predictions\n",
                "    item_B = item.copy()\n",
                "    item_B[\"contexts\"] = formatted_contexts\n",
                "    item_B[\"predictions\"] = formatted_predictions\n",
                "    subs_B.append(item_B)\n",
                "    \n",
                "    # Task C: Contexts + Predictions\n",
                "    item_C = item.copy()\n",
                "    item_C[\"contexts\"] = formatted_contexts\n",
                "    item_C[\"predictions\"] = formatted_predictions\n",
                "    subs_C.append(item_C)\n",
                "\n",
                "print(f\"Finished. Processed {len(subs_A)} items.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4756cc81",
            "metadata": {},
            "source": [
                "## 3. Save Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5df4eb75",
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_jsonl(data: List[Dict], filename: str):\n",
                "    path = os.path.join(OUTPUT_DIR, filename)\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for x in data:\n",
                "            json.dump(x, f)\n",
                "            f.write('\\n')\n",
                "    print(f\"Saved: {path}\")\n",
                "    return path\n",
                "\n",
                "file_A = save_jsonl(subs_A, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "file_B = save_jsonl(subs_B, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "file_C = save_jsonl(subs_C, f\"submission_TaskC_{TEAM_NAME}.jsonl\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bdf5d340",
            "metadata": {},
            "source": [
                "## 4. Validate All"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75320243",
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_task_format(file_path: str, task_type: str):\n",
                "    try:\n",
                "        with open(file_path, 'r') as f:\n",
                "            data = [json.loads(line) for line in f if line.strip()]\n",
                "        if not data: return False, \"Empty\"\n",
                "        sample = data[0]\n",
                "        errors = []\n",
                "\n",
                "        # Check Requirements\n",
                "        # Task A/C need contexts\n",
                "        if task_type in [\"TaskA\", \"TaskC\"]:\n",
                "            if \"contexts\" not in sample: errors.append(\"Missing 'contexts'\")\n",
                "            elif not isinstance(sample[\"contexts\"], list): errors.append(\"'contexts' not list\")\n",
                "\n",
                "        # Task B/C need predictions\n",
                "        if task_type in [\"TaskB\", \"TaskC\"]:\n",
                "            if \"predictions\" not in sample: errors.append(\"Missing 'predictions'\")\n",
                "            elif not isinstance(sample[\"predictions\"], list): errors.append(\"'predictions' not list\")\n",
                "        \n",
                "        if errors: return False, \"; \".join(errors)\n",
                "        return True, \"OK\"\n",
                "    except Exception as e:\n",
                "        return False, str(e)\n",
                "\n",
                "print(\"--- Validation ---\")\n",
                "for task, fpath in [(\"TaskA\", file_A), (\"TaskB\", file_B), (\"TaskC\", file_C)]:\n",
                "    valid, msg = validate_task_format(fpath, task)\n",
                "    status = \"\\033[92mPASS\\033[0m\" if valid else \"\\033[91mFAIL\\033[0m\"\n",
                "    print(f\"[{task}] {status} : {msg}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}