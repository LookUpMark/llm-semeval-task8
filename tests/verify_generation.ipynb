{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Generation Module Verification\n",
                "\n",
                "This notebook verifies the functionality of **ALL** components in `src.generation`:\n",
                "1.  **Generator** (Llama 3.1 8B or 3.2 3B 4-bit)\n",
                "2.  **Query Rewriter**\n",
                "3.  **Retrieval Grader** (CRAG)\n",
                "4.  **Hallucination Grader** (Self-RAG)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup and Imports\n",
                "First, we set up the environment and import necessary libraries. \n",
                "We append the project root to `sys.path` to allow importing modules from `src/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project Root added: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "# from tqdm.notebook import tqdm # Use this if strict notebook, but standard print is safer for mixed envs\n",
                "\n",
                "# Ensure we can import from src\n",
                "# Assuming this notebook is running from 'tests/'\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "print(f\"Project Root added: {project_root}\")\n",
                "\n",
                "from src.generation import create_generation_components"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration\n",
                "Select the model ID to use. \n",
                "- Use `meta-llama/Llama-3.2-3B-Instruct` for faster local testing.\n",
                "- Use `meta-llama/Meta-Llama-3.1-8B-Instruct` for production-grade testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CONFIGURATION\n",
                "# MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # Standard\n",
                "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\" # Light for local testing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Classes\n",
                "Define helper classes for colored terminal output to make the verification results easier to read."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- HELPER CLASS FOR STYLED OUTPUT ---\n",
                "\n",
                "class Colors:\n",
                "    HEADER = '\\033[95m'\n",
                "    BLUE = '\\033[94m'\n",
                "    CYAN = '\\033[96m'\n",
                "    GREEN = '\\033[92m'\n",
                "    YELLOW = '\\033[93m'\n",
                "    RED = '\\033[91m'\n",
                "    BOLD = '\\033[1m'\n",
                "    UNDERLINE = '\\033[4m'\n",
                "    RESET = '\\033[0m'\n",
                "\n",
                "def print_header(title):\n",
                "    print(f\"\\n{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.RESET}\")\n",
                "    print(f\"{Colors.HEADER}{Colors.BOLD} {title} {Colors.RESET}\")\n",
                "    print(f\"{Colors.HEADER}{Colors.BOLD}{'='*60}{Colors.RESET}\\n\")\n",
                "\n",
                "def print_section(title):\n",
                "    print(f\"\\n{Colors.CYAN}{Colors.BOLD}>>> {title} {Colors.RESET}\")\n",
                "\n",
                "def print_input(label, content):\n",
                "    print(f\"{Colors.YELLOW}{Colors.BOLD}{label}:{Colors.RESET} {content}\")\n",
                "\n",
                "def print_output(output, expected):\n",
                "    print(f\"{Colors.GREEN}{Colors.BOLD}Output:{Colors.RESET} {output}\")\n",
                "    print(f\"{Colors.BLUE}{Colors.BOLD}Expected:{Colors.RESET} {expected}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Component Initialization\n",
                "Initialize the generation components using the factory function. This loads the model (quantized) and sets up the LangChain pipelines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[95m\u001b[1m============================================================\u001b[0m\n",
                        "\u001b[95m\u001b[1m INITIALIZATION \u001b[0m\n",
                        "\u001b[95m\u001b[1m============================================================\u001b[0m\n",
                        "\n",
                        "Creating Generation Components with model: meta-llama/Llama-3.2-3B-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.14s/it]\n",
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generation Components Ready.\n"
                    ]
                }
            ],
            "source": [
                "print_header(\"INITIALIZATION\")\n",
                "components = create_generation_components(model_id=MODEL_ID)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test Generator\n",
                "The Generator is responsible for answering questions based on the provided context.\n",
                "\n",
                "### 1.1 Positive Case\n",
                "We provide a context that contains the answer. The model should correctly extract the information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Generator (Positive) \u001b[0m\n",
                        "\u001b[93m\u001b[1mContext:\u001b[0m Tim Cook is the CEO of Apple.\n",
                        "\u001b[93m\u001b[1mQuestion:\u001b[0m Who is the CEO of Apple?\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m Tim Cook\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m Something like 'Tim Cook'\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Generator (Positive)\")\n",
                "context = \"Tim Cook is the CEO of Apple.\"\n",
                "question = \"Who is the CEO of Apple?\"\n",
                "\n",
                "print_input(\"Context\", context)\n",
                "print_input(\"Question\", question)\n",
                "\n",
                "res = components.generator.invoke({\"context\": context, \"question\": question})\n",
                "print_output(res.strip(), \"Something like 'Tim Cook'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Negative Case (Fallback)\n",
                "We provide a context that is irrelevant to the question. The model **must** strictly reply with `I_DONT_KNOW`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Generator (Negative) \u001b[0m\n",
                        "\u001b[93m\u001b[1mContext:\u001b[0m The sky is blue.\n",
                        "\u001b[93m\u001b[1mQuestion:\u001b[0m Who is the CEO of Apple?\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m I_DONT_KNOW\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m 'I_DONT_KNOW'\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Generator (Negative)\")\n",
                "context = \"The sky is blue.\"\n",
                "question = \"Who is the CEO of Apple?\"\n",
                "\n",
                "print_input(\"Context\", context)\n",
                "print_input(\"Question\", question)\n",
                "\n",
                "res = components.generator.invoke({\"context\": context, \"question\": question})\n",
                "print_output(res.strip(), \"'I_DONT_KNOW'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test Query Rewriter\n",
                "The Query Rewriter transforms a user question that might depend on previous chat history (e.g., \"When was *he* born?\") into a standalone question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Query Rewriter \u001b[0m\n",
                        "\u001b[93m\u001b[1mHistory:\u001b[0m [('human', 'Who is the creator of Python?'), ('assistant', 'Guido van Rossum')]\n",
                        "\u001b[93m\u001b[1mQuestion:\u001b[0m When was he born?\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m When was Guido van Rossum born?\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m 'When was Guido van Rossum born?'\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Query Rewriter\")\n",
                "history = [\n",
                "    (\"human\", \"Who is the creator of Python?\"),\n",
                "    ('assistant', 'Guido van Rossum')\n",
                "]\n",
                "question = \"When was he born?\"\n",
                "\n",
                "print_input(\"History\", str(history))\n",
                "print_input(\"Question\", question)\n",
                "\n",
                "res = components.query_rewriter.invoke({\"messages\": history, \"question\": question})\n",
                "print_output(res.strip(), \"'When was Guido van Rossum born?'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Test Retrieval Grader (CRAG)\n",
                "This component evaluates whether a retrieved document is relevant to the question. It returns a JSON with a binary score.\n",
                "\n",
                "### 3.1 Relevant Document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Retrieval Grader (Relevant) \u001b[0m\n",
                        "\u001b[93m\u001b[1mQuestion:\u001b[0m What is the capital of France?\n",
                        "\u001b[93m\u001b[1mDocument:\u001b[0m Paris is the capital of France.\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m {'binary_score': 'yes'}\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m {'binary_score': 'yes'}\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Retrieval Grader (Relevant)\")\n",
                "question = \"What is the capital of France?\"\n",
                "document = \"Paris is the capital of France.\"\n",
                "\n",
                "print_input(\"Question\", question)\n",
                "print_input(\"Document\", document)\n",
                "\n",
                "res = components.retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
                "print_output(res, \"{'binary_score': 'yes'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Irrelevant Document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Retrieval Grader (Irrelevant) \u001b[0m\n",
                        "\u001b[93m\u001b[1mQuestion:\u001b[0m What is the capital of France?\n",
                        "\u001b[93m\u001b[1mDocument:\u001b[0m To make pizza you need flour.\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m {'binary_score': 'no'}\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m {'binary_score': 'no'}\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Retrieval Grader (Irrelevant)\")\n",
                "question = \"What is the capital of France?\"\n",
                "document = \"To make pizza you need flour.\"\n",
                "\n",
                "print_input(\"Question\", question)\n",
                "print_input(\"Document\", document)\n",
                "\n",
                "res = components.retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
                "print_output(res, \"{'binary_score': 'no'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Hallucination Grader (Self-RAG)\n",
                "This component verifies if the generated answer is supported by the retrieved facts (groundedness).\n",
                "\n",
                "### 4.1 Grounded Answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Hallucination Grader (Grounded) \u001b[0m\n",
                        "\u001b[93m\u001b[1mDocuments:\u001b[0m The Python programming language was created by Guido van Rossum.\n",
                        "\u001b[93m\u001b[1mGeneration:\u001b[0m Guido van Rossum created Python.\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m {'binary_score': 'yes'}\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m {'binary_score': 'yes'}\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Hallucination Grader (Grounded)\")\n",
                "documents = \"The Python programming language was created by Guido van Rossum.\"\n",
                "generation = \"Guido van Rossum created Python.\"\n",
                "\n",
                "print_input(\"Documents\", documents)\n",
                "print_input(\"Generation\", generation)\n",
                "\n",
                "res = components.hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
                "print_output(res, \"{'binary_score': 'yes'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Hallucinated Answer\n",
                "The answer is not supported by the document provided."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[96m\u001b[1m>>> TEST: Hallucination Grader (Hallucination) \u001b[0m\n",
                        "\u001b[93m\u001b[1mDocuments:\u001b[0m The Python programming language was created by Guido van Rossum.\n",
                        "\u001b[93m\u001b[1mGeneration:\u001b[0m Elon Musk created Python.\n",
                        "\u001b[92m\u001b[1mOutput:\u001b[0m {'binary_score': 'no'}\n",
                        "\u001b[94m\u001b[1mExpected:\u001b[0m {'binary_score': 'no'}\n"
                    ]
                }
            ],
            "source": [
                "print_section(\"TEST: Hallucination Grader (Hallucination)\")\n",
                "documents = \"The Python programming language was created by Guido van Rossum.\"\n",
                "generation = \"Elon Musk created Python.\"\n",
                "\n",
                "print_input(\"Documents\", documents)\n",
                "print_input(\"Generation\", generation)\n",
                "\n",
                "res = components.hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
                "print_output(res, \"{'binary_score': 'no'}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
