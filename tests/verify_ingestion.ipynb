{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Ingestion Module Verification\n",
    "\n",
    "This notebook tests the `src.ingestion` module which handles:\n",
    "- **Loading mtRAG data** from JSONL files\n",
    "- **Parent-Child Chunking** for optimal retrieval (large chunks for context, small for search)\n",
    "- **Vector Store Creation** using Qdrant + BGE-M3 embeddings\n",
    "\n",
    "Uses a **small subset (50 docs)** for fast testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_ingestion_test\")\n",
    "MAX_DOCS = 50\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Test subset size: {MAX_DOCS} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prep-section",
   "metadata": {},
   "source": [
    "## Step 0: Prepare Test Data\n",
    "Extract the corpus and create a small subset for fast testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract corpus if needed\n",
    "corpus_dir = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
    "jsonl_file = os.path.join(corpus_dir, \"govt.jsonl\")\n",
    "zip_file = os.path.join(corpus_dir, \"govt.jsonl.zip\")\n",
    "\n",
    "if not os.path.exists(jsonl_file) and os.path.exists(zip_file):\n",
    "    print(\"Extracting corpus...\")\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        zf.extractall(corpus_dir)\n",
    "    print(\"Corpus extracted\")\n",
    "else:\n",
    "    print(f\"Corpus ready: govt.jsonl\")\n",
    "\n",
    "# Create test subset\n",
    "test_file = os.path.join(PROJECT_ROOT, \"data/test_subset.jsonl\")\n",
    "os.makedirs(os.path.dirname(test_file), exist_ok=True)\n",
    "\n",
    "print(f\"Creating test subset with {MAX_DOCS} documents...\")\n",
    "with open(jsonl_file, 'r') as f_in, open(test_file, 'w') as f_out:\n",
    "    for i, line in enumerate(f_in):\n",
    "        if i >= MAX_DOCS:\n",
    "            break\n",
    "        f_out.write(line)\n",
    "print(f\"Test file created: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-section",
   "metadata": {},
   "source": [
    "## Step 1: Test `load_and_chunk_data()`\n",
    "\n",
    "This function:\n",
    "1. Loads documents from JSONL\n",
    "2. Applies **Parent-Child Chunking**:\n",
    "   - Parent chunks: 1200 chars (full context for LLM)\n",
    "   - Child chunks: 400 chars (indexed for search)\n",
    "3. Stores parent text in child metadata for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion import load_and_chunk_data\n",
    "\n",
    "print(\"Loading and chunking data...\")\n",
    "docs = load_and_chunk_data(test_file)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"   â€¢ Total chunks created: {len(docs)}\")\n",
    "print(f\"   â€¢ Avg chunks per document: {len(docs) / MAX_DOCS:.1f}\")\n",
    "\n",
    "print(f\"\\nðŸ“„ Sample chunk:\")\n",
    "sample = docs[0]\n",
    "print(f\"   â€¢ Child content (indexed): {sample.page_content[:100]}...\")\n",
    "print(f\"   â€¢ Parent text length: {len(sample.metadata.get('parent_text', ''))} chars\")\n",
    "print(f\"   â€¢ Metadata keys: {list(sample.metadata.keys())}\")\n",
    "\n",
    "print(\"\\nload_and_chunk_data() working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorstore-section",
   "metadata": {},
   "source": [
    "## Step 2: Test `build_vector_store()`\n",
    "\n",
    "This function:\n",
    "1. Creates HuggingFace embeddings (BGE-M3)\n",
    "2. Initializes Qdrant local database\n",
    "3. Indexes all chunks with their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-vectorstore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion import build_vector_store\n",
    "\n",
    "# Use only first 30 chunks for speed\n",
    "docs_subset = docs[:30]\n",
    "print(f\"Building vector store with {len(docs_subset)} chunks...\")\n",
    "print(\"   (Using subset for faster testing)\")\n",
    "\n",
    "vectorstore = build_vector_store(docs_subset, persist_dir=QDRANT_PATH)\n",
    "\n",
    "print(\"\\nbuild_vector_store() working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-section",
   "metadata": {},
   "source": [
    "## Step 3: Verify Qdrant Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = vectorstore.client.get_collection(\"mtrag_collection\")\n",
    "\n",
    "print(f\"Collection Statistics:\")\n",
    "print(f\"   â€¢ Points (vectors): {info.points_count}\")\n",
    "print(f\"   â€¢ Status: {info.status}\")\n",
    "\n",
    "print(\"\\nCollection created and verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search-section",
   "metadata": {},
   "source": [
    "## Step 4: Test Similarity Search\n",
    "\n",
    "Verify that the vector store can find relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"government regulations\"\n",
    "print(f\"Testing search with query: '{query}'\")\n",
    "\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"\\nFound {len(results)} results:\")\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\n   Result {i+1} (similarity: {score:.4f}):\")\n",
    "    print(f\"   â€¢ Child chunk: {doc.page_content[:80]}...\")\n",
    "    parent = doc.metadata.get('parent_text', '')\n",
    "    print(f\"   â€¢ Parent context: {parent[:80]}...\" if parent else \"   â€¢ No parent\")\n",
    "\n",
    "print(\"\\nSimilarity search working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-section",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Remove test files after verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Close client first\n",
    "vectorstore.client.close()\n",
    "\n",
    "# Remove test files\n",
    "if os.path.exists(QDRANT_PATH):\n",
    "    shutil.rmtree(QDRANT_PATH)\n",
    "    print(f\"Removed test database: {QDRANT_PATH}\")\n",
    "\n",
    "if os.path.exists(test_file):\n",
    "    os.remove(test_file)\n",
    "    print(f\"Removed test subset: {test_file}\")\n",
    "\n",
    "print(\"\\nAll ingestion tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}