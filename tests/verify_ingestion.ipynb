{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Data Ingestion Module Verification\n",
                "\n",
                "This notebook tests the `src.ingestion` module which handles:\n",
                "- **Loading mtRAG data** from JSONL files\n",
                "- **Parent-Child Chunking** for optimal retrieval (large chunks for context, small for search)\n",
                "- **Vector Store Creation** using Qdrant + BGE-M3 embeddings\n",
                "\n",
                "Uses a **small subset (50 docs)** for fast testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "setup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project root: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n",
                        "Test subset size: 50 documents\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import zipfile\n",
                "\n",
                "sys.path.append(os.path.abspath(\"..\"))\n",
                "PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_ingestion_test\")\n",
                "MAX_DOCS = 50\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")\n",
                "print(f\"Test subset size: {MAX_DOCS} documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prep-section",
            "metadata": {},
            "source": [
                "## Step 0: Prepare Test Data\n",
                "Extract the corpus and create a small subset for fast testing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "prepare-data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Corpus ready: govt.jsonl\n",
                        "üìù Creating test subset with 50 documents...\n",
                        "‚úÖ Test file created: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/test_subset.jsonl\n"
                    ]
                }
            ],
            "source": [
                "# Extract corpus if needed\n",
                "corpus_dir = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "jsonl_file = os.path.join(corpus_dir, \"govt.jsonl\")\n",
                "zip_file = os.path.join(corpus_dir, \"govt.jsonl.zip\")\n",
                "\n",
                "if not os.path.exists(jsonl_file) and os.path.exists(zip_file):\n",
                "    print(\"üì¶ Extracting corpus...\")\n",
                "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
                "        zf.extractall(corpus_dir)\n",
                "    print(\"‚úÖ Corpus extracted\")\n",
                "else:\n",
                "    print(f\"‚úÖ Corpus ready: govt.jsonl\")\n",
                "\n",
                "# Create test subset\n",
                "test_file = os.path.join(PROJECT_ROOT, \"data/test_subset.jsonl\")\n",
                "os.makedirs(os.path.dirname(test_file), exist_ok=True)\n",
                "\n",
                "print(f\"üìù Creating test subset with {MAX_DOCS} documents...\")\n",
                "with open(jsonl_file, 'r') as f_in, open(test_file, 'w') as f_out:\n",
                "    for i, line in enumerate(f_in):\n",
                "        if i >= MAX_DOCS:\n",
                "            break\n",
                "        f_out.write(line)\n",
                "print(f\"‚úÖ Test file created: {test_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-section",
            "metadata": {},
            "source": [
                "## Step 1: Test `load_and_chunk_data()`\n",
                "\n",
                "This function:\n",
                "1. Loads documents from JSONL\n",
                "2. Applies **Parent-Child Chunking**:\n",
                "   - Parent chunks: 1200 chars (full context for LLM)\n",
                "   - Child chunks: 400 chars (indexed for search)\n",
                "3. Stores parent text in child metadata for retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "test-load",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Loading and chunking data...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/test_subset.jsonl ---\n",
                        "Loaded 50 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "\n",
                        "üìä Results:\n",
                        "   ‚Ä¢ Total chunks created: 409\n",
                        "   ‚Ä¢ Avg chunks per document: 8.2\n",
                        "\n",
                        "üìÑ Sample chunk:\n",
                        "   ‚Ä¢ Child content (indexed): FAQs ‚Ä¢ What type of seats can I book?\n",
                        "FAQs ‚Ä¢ What type of seats can I book?\n",
                        "\n",
                        " \n",
                        "\n",
                        "Skip to Main Content...\n",
                        "   ‚Ä¢ Parent text length: 776 chars\n",
                        "   ‚Ä¢ Metadata keys: ['doc_id', 'title', 'url', 'source', 'parent_text', 'parent_title', 'parent_url', 'parent_id']\n",
                        "\n",
                        "‚úÖ load_and_chunk_data() working correctly!\n"
                    ]
                }
            ],
            "source": [
                "from src.ingestion import load_and_chunk_data\n",
                "\n",
                "print(\"üîÑ Loading and chunking data...\")\n",
                "docs = load_and_chunk_data(test_file)\n",
                "\n",
                "print(f\"\\nüìä Results:\")\n",
                "print(f\"   ‚Ä¢ Total chunks created: {len(docs)}\")\n",
                "print(f\"   ‚Ä¢ Avg chunks per document: {len(docs) / MAX_DOCS:.1f}\")\n",
                "\n",
                "print(f\"\\nüìÑ Sample chunk:\")\n",
                "sample = docs[0]\n",
                "print(f\"   ‚Ä¢ Child content (indexed): {sample.page_content[:100]}...\")\n",
                "print(f\"   ‚Ä¢ Parent text length: {len(sample.metadata.get('parent_text', ''))} chars\")\n",
                "print(f\"   ‚Ä¢ Metadata keys: {list(sample.metadata.keys())}\")\n",
                "\n",
                "print(\"\\n‚úÖ load_and_chunk_data() working correctly!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vectorstore-section",
            "metadata": {},
            "source": [
                "## Step 2: Test `build_vector_store()`\n",
                "\n",
                "This function:\n",
                "1. Creates HuggingFace embeddings (BGE-M3)\n",
                "2. Initializes Qdrant local database\n",
                "3. Indexes all chunks with their embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "test-vectorstore",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Building vector store with 30 chunks...\n",
                        "   (Using subset for faster testing)\n",
                        "--- BUILDING VECTOR STORE ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/src/ingestion.py:108: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  embedding_model = HuggingFaceEmbeddings(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- VECTOR STORE BUILT AND SAVED ---\n",
                        "\n",
                        "‚úÖ build_vector_store() working correctly!\n"
                    ]
                }
            ],
            "source": [
                "from src.ingestion import build_vector_store\n",
                "\n",
                "# Use only first 30 chunks for speed\n",
                "docs_subset = docs[:30]\n",
                "print(f\"üîÑ Building vector store with {len(docs_subset)} chunks...\")\n",
                "print(\"   (Using subset for faster testing)\")\n",
                "\n",
                "vectorstore = build_vector_store(docs_subset, persist_dir=QDRANT_PATH)\n",
                "\n",
                "print(\"\\n‚úÖ build_vector_store() working correctly!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "verify-section",
            "metadata": {},
            "source": [
                "## Step 3: Verify Qdrant Collection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "verify-collection",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Collection Statistics:\n",
                        "   ‚Ä¢ Points (vectors): 30\n",
                        "   ‚Ä¢ Status: green\n",
                        "\n",
                        "‚úÖ Collection created and verified!\n"
                    ]
                }
            ],
            "source": [
                "info = vectorstore.client.get_collection(\"mtrag_collection\")\n",
                "\n",
                "print(f\"üìä Collection Statistics:\")\n",
                "print(f\"   ‚Ä¢ Points (vectors): {info.points_count}\")\n",
                "print(f\"   ‚Ä¢ Status: {info.status}\")\n",
                "\n",
                "print(\"\\n‚úÖ Collection created and verified!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "search-section",
            "metadata": {},
            "source": [
                "## Step 4: Test Similarity Search\n",
                "\n",
                "Verify that the vector store can find relevant documents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "test-search",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Testing search with query: 'government regulations'\n",
                        "\n",
                        "üìä Found 3 results:\n",
                        "\n",
                        "   Result 1 (similarity: 0.4917):\n",
                        "   ‚Ä¢ Child chunk: Adopted City Council Ordinances\n",
                        "\n",
                        "Adopted City Council Resolutions\n",
                        "\n",
                        " \n",
                        "\n",
                        " \n",
                        "HomeFAQs...\n",
                        "   ‚Ä¢ Parent context: FAQs ‚Ä¢ What type of seats can I book?\n",
                        "FAQs ‚Ä¢ What type of seats can I book?\n",
                        "\n",
                        " \n",
                        "\n",
                        "...\n",
                        "\n",
                        "   Result 2 (similarity: 0.4531):\n",
                        "   ‚Ä¢ Child chunk: FAQs ‚Ä¢ What type of seats can I book?\n",
                        "FAQs ‚Ä¢ What type of seats can I book?\n",
                        "\n",
                        " \n",
                        "\n",
                        "...\n",
                        "   ‚Ä¢ Parent context: FAQs ‚Ä¢ What type of seats can I book?\n",
                        "FAQs ‚Ä¢ What type of seats can I book?\n",
                        "\n",
                        " \n",
                        "\n",
                        "...\n",
                        "\n",
                        "   Result 3 (similarity: 0.4520):\n",
                        "   ‚Ä¢ Child chunk: 3.\n",
                        "What type of seats can I book?...\n",
                        "   ‚Ä¢ Parent context: 2.\n",
                        "Where can I board MoGo?\n",
                        "\n",
                        "Pick-up and drop-off is available at designated virt...\n",
                        "\n",
                        "‚úÖ Similarity search working correctly!\n"
                    ]
                }
            ],
            "source": [
                "query = \"government regulations\"\n",
                "print(f\"üîç Testing search with query: '{query}'\")\n",
                "\n",
                "results = vectorstore.similarity_search_with_score(query, k=3)\n",
                "\n",
                "print(f\"\\nüìä Found {len(results)} results:\")\n",
                "for i, (doc, score) in enumerate(results):\n",
                "    print(f\"\\n   Result {i+1} (similarity: {score:.4f}):\")\n",
                "    print(f\"   ‚Ä¢ Child chunk: {doc.page_content[:80]}...\")\n",
                "    parent = doc.metadata.get('parent_text', '')\n",
                "    print(f\"   ‚Ä¢ Parent context: {parent[:80]}...\" if parent else \"   ‚Ä¢ No parent\")\n",
                "\n",
                "print(\"\\n‚úÖ Similarity search working correctly!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cleanup-section",
            "metadata": {},
            "source": [
                "## Cleanup\n",
                "Remove test files after verification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "cleanup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üóëÔ∏è Removed test database: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/qdrant_ingestion_test\n",
                        "üóëÔ∏è Removed test subset: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/test_subset.jsonl\n",
                        "\n",
                        "‚úÖ All ingestion tests passed!\n"
                    ]
                }
            ],
            "source": [
                "import shutil\n",
                "\n",
                "# Close client first\n",
                "vectorstore.client.close()\n",
                "\n",
                "# Remove test files\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    shutil.rmtree(QDRANT_PATH)\n",
                "    print(f\"üóëÔ∏è Removed test database: {QDRANT_PATH}\")\n",
                "\n",
                "if os.path.exists(test_file):\n",
                "    os.remove(test_file)\n",
                "    print(f\"üóëÔ∏è Removed test subset: {test_file}\")\n",
                "\n",
                "print(\"\\n‚úÖ All ingestion tests passed!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
