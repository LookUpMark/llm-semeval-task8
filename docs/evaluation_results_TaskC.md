# Task C Evaluation Results - Post-Fix Analysis

**Date:** 31 gennaio 2026  
**Submission File:** `submission_TaskC_Gbgers.jsonl`  
**Ground Truth:** `dataset/human/evaluations/reference.json`  
**Team:** Gbgers

---

## Methodology

### Metrics Used
- **Semantic Similarity:** Cosine similarity using `all-MiniLM-L6-v2` embeddings between generated answer and ground truth
- **Faithfulness (1-10):** Semantic alignment between answer and retrieved context (grounding)
- **Correctness (1-10):** Semantic alignment with reference answer √ó 10

### Evaluation Logic
- **Refusal Detection:** Response contains "I_DONT_KNOW" or phrases like "i cannot", "unable to", etc.
- **Ground Truth Answerability:** From `reference.json` ‚Üí `Answerability: ["UNANSWERABLE"]`
- **Correct Refusal:** We refused AND GT says UNANSWERABLE
- **False Refusal:** We refused BUT GT says ANSWERABLE (bad!)
- **Answered Unanswerable:** We answered BUT GT says UNANSWERABLE (hallucination risk)

---

## Aggregate Results

| Metric | Value |
|--------|-------|
| **Total Samples** | 110 |
| **Total Refusals** | 77 (70.0%) |
| **Total Answered** | 33 (30.0%) |

### Refusal Breakdown

| Type | Count | Interpretation |
|------|-------|----------------|
| ‚úÖ Correct Refusals | 6 | GT = UNANSWERABLE, we correctly refused |
| ‚ùå False Refusals | 71 | GT = ANSWERABLE, we wrongly refused |

### Answer Breakdown

| Type | Count | Interpretation |
|------|-------|----------------|
| ‚ùå Answered Unanswerable | 2 | GT = UNANSWERABLE but we answered (hallucination) |
| ‚úÖ Good Answers | 1 | Faith ‚â• 5 AND Corr ‚â• 5 |
| ‚ö†Ô∏è Needs Check | 30 | Faith < 5 OR Corr < 5 |

### Quality Metrics (Answerable Questions Only, n=31)

| Metric | Value |
|--------|-------|
| Avg Semantic Similarity | 0.1536 |
| Avg Faithfulness (1-10) | 4.49 |
| Avg Correctness (1-10) | 1.54 |

---

## Fallback Reason Distribution

| Reason | Count | % of Refusals |
|--------|-------|---------------|
| `llm_refusal` | 42 | 54.5% |
| `hallucination_loop_exhausted` | 19 | 24.7% |
| `irrelevant_docs` | 16 | 20.8% |

---

## Per-Domain Summary

| Domain | Refusals | Correct | False | Answered | Good |
|--------|----------|---------|-------|----------|------|
| **GOVT** | 22 | 1 | 21 | 6 | 0 |
| **CLAPNQ** | 25 | 4 | 21 | 4 | 0 |
| **FIQA** | 14 | 1 | 13 | 13 | 0 |
| **CLOUD** | 16 | 0 | 16 | 10 | 1 |

---

## Critical Observations

### üî¥ Major Problems

1. **False Refusal Rate: 64.5%** (71/110)
   - Il sistema rifiuta di rispondere a domande che HANNO una risposta nel ground truth
   - La causa principale √® `llm_refusal` (42 casi) ‚Üí il modello si auto-censura troppo

2. **Extremely Low Correctness: 1.54/10**
   - Le risposte date non sono allineate semanticamente alle reference answer
   - Media semantic similarity = 0.15 (praticamente ortogonali)

3. **Solo 1 "Good Answer" su 110**
   - Solo cloud_3 supera entrambe le soglie (faith‚â•5, corr‚â•5)

### üü° Secondary Issues

4. **Hallucination Loop Exhausted: 19 casi**
   - Il Self-CRAG rileva hallucination e va in loop fino a esaurimento retry
   - Dopo 3 tentativi ‚Üí fallback a I_DONT_KNOW

5. **Irrelevant Docs: 16 casi**
   - Il relevance grader giudica i documenti non pertinenti
   - Possibile problema di retrieval o soglia troppo alta

### üü¢ Positive Aspects

6. **Solo 2 Answered Unanswerable**
   - Il sistema √® conservativo: quando risponde, tende ad avere contesto
   - Ma forse troppo conservativo

---

## Root Cause Analysis

### Problema 1: LLM Refusal (42 casi)
Il prompt di generazione contiene:
```
If the documents lack the information needed to answer the question, respond with "I_DONT_KNOW"
```
Con il prompt "conservative" applicato durante i fix, il modello √® diventato **troppo prudente** e preferisce rifiutare.

### Problema 2: Hallucination Grader Troppo Stringente
Il grader modificato richiede che:
```
Every key fact in the answer MUST be explicitly stated in the documents
```
Questo √® troppo restrittivo per risposte che richiedono ragionamento o sintesi.

### Problema 3: Domain Filter Mismatch
Durante i fix √® stato corretto il filtro da `metadata.domain` a `metadata.source`, ma questo potrebbe aver peggiorato il recall se il campo `source` ha valori diversi da quelli attesi.

---

## Recommendations for Next Submission

1. **Rilassare il Prompt di Generazione**
   - Rimuovere "I_DONT_KNOW" come opzione esplicita
   - Usare "Answer based on the provided context"

2. **Abbassare Soglia Hallucination Grader**
   - Da "strict: every key fact MUST be explicitly stated"
   - A "the answer should be generally consistent with the documents"

3. **Verificare Domain Filter**
   - Controllare i valori effettivi di `metadata.source` nel Qdrant
   - Considerare di disabilitare il filtro domain (`use_filter=False`)

4. **Aumentare Max Retries**
   - Da 3 a 5 per dare pi√π chance al Self-CRAG

5. **Debug sul Retrieval**
   - I 16 casi `irrelevant_docs` suggeriscono problemi di retrieval
   - Verificare che i documenti siano effettivamente indicizzati per i domini corretti

---

## Comparison: Pre-Fix vs Post-Fix

### Riepilogo Fix Applicati

| Fix | Descrizione | Impatto Atteso |
|-----|-------------|----------------|
| Domain Filter | `metadata.domain` ‚Üí `metadata.source` | ‚úÖ Retrieval corretto per dominio |
| Conservative Prompt | "MUST be directly supported" | ‚ùå Troppo restrittivo |
| Strict Hallucination Grader | "Every key fact MUST be explicit" | ‚ùå Troppo stringente |
| Dual Query Retrieval | Original + Standalone query | ‚ö†Ô∏è Non sufficiente |
| Fallback Cascade | Filter OFF se 0 risultati | ‚ö†Ô∏è Mitigazione parziale |

### Verdetto Finale

**I risultati post-fix sono PEGGIORI di quanto ci si aspettasse.**

La submission precedente (presumibilmente) aveva un tasso di refusal pi√π basso, anche se potenzialmente con pi√π hallucination. I fix applicati hanno:

1. **Corretto** il bug del domain filter ‚Üí retrieval pi√π accurato
2. **Peggiorato** la recall ‚Üí il sistema rifiuta troppo spesso (70%)
3. **Peggiorato** la qualit√† delle risposte ‚Üí solo 1/110 √® "good"

### Causa Principale del Fallimento

Il problema NON era l'hallucination rate alto come suggerito dall'altra AI, ma piuttosto:

1. **Il modello Llama 3.1 8B quantizzato a 4-bit** non ha sufficiente capacit√† per generare risposte fedeli
2. **Il Self-CRAG crea un loop** dove il grader rileva "hallucination" anche su risposte corrette
3. **I prompt "conservative"** hanno reso il modello troppo prudente

### Affidabilit√† dei Risultati

| Aspetto | Pre-Fix (stimato) | Post-Fix | Differenza |
|---------|-------------------|----------|------------|
| Refusal Rate | ~30-40% | 70% | ‚¨ÜÔ∏è Peggio |
| False Refusals | ~20-30% | 64.5% | ‚¨ÜÔ∏è Molto peggio |
| Avg Correctness | ~2-3/10 | 1.54/10 | ‚¨áÔ∏è Peggio |
| Hallucination | Alto (?) | Basso (ma irrilevante perch√© rifiuta tutto) | - |

---

## Comparison: Strictness vs Model/Docs

### Due configurazioni confrontate

| Aspetto | Config precedente (meno strict) | Config attuale (pi√π strict) | Effetto osservato |
|---------|--------------------------------|-----------------------------|-------------------|
| Prompt generazione | Meno restrittivo | "MUST be directly supported" | ‚¨ÜÔ∏è Refusal rate |
| Hallucination grader | Lax/moderato | "Every key fact MUST be explicit" | ‚¨ÜÔ∏è Loop + fallback |
| Risposte | Pi√π frequenti | Molto rare | ‚¨áÔ∏è Coverage |
| Hallucination | Potenzialmente pi√π alta | Pi√π bassa | ‚úÖ ma irrilevante |

### Evidenza che il problema √® modello + documenti

1. **Modello sottodimensionato**
   - Llama 3.1 8B 4-bit mostra bassa accuratezza semantica anche quando risponde
   - Avg Correctness = **1.54/10** e Avg Semantic Similarity = **0.1536** ‚Üí problema di capacit√†

2. **Documenti mancanti o non recuperati**
   - Refusals con `irrelevant_docs` + `hallucination_loop_exhausted` = **35/77**
   - Questo indica che spesso il retriever NON porta contesto utile

3. **Strictness non √® la causa primaria**
   - Stringere i prompt riduce le hallucination ma **non migliora la correttezza**
   - L‚Äôaccuratezza rimane bassa ‚Üí limite del modello e/o dei documenti

### Conclusione della comparison

**La differenza di strictness cambia il tasso di refusal, non la qualit√† reale delle risposte.**
Il collo di bottiglia √® la combinazione di:

- **modello troppo piccolo** per risposte multi-turn e grounding
- **documenti non sufficienti o retrieval insufficiente**

---

## Conclusione

**I risultati NON sono migliori n√© pi√π affidabili.**

L'approccio conservativo ha fallito. Per migliorare serve:
1. Rimuovere o rilassare significativamente il Self-CRAG
2. Usare un modello pi√π grande (13B+) o non quantizzato
3. Verificare che l'indice Qdrant contenga effettivamente i documenti corretti
4. Considerare un approccio pi√π semplice: retrieval + generazione diretta senza CRAG

