{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Complete Pipeline for All Tasks\n",
                "\n",
                "| Task | Description | Output |\n",
                "|------|-------------|--------|\n",
                "| **A** | Document Retrieval | Top-K passages |\n",
                "| **B** | Standalone Generation | LLM-only answer |\n",
                "| **C** | RAG Generation | Context + LLM answer |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "kaggle_env",
            "metadata": {},
            "outputs": [],
            "source": [
                "# KAGGLE SETUP (uncomment on Kaggle)\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -q langchain langchain-community langchain-huggingface langchain-qdrant qdrant-client sentence-transformers bitsandbytes accelerate transformers tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n"
                    ]
                }
            ],
            "source": [
                "import os, sys, json, zipfile\n",
                "from tqdm import tqdm\n",
                "\n",
                "if os.path.exists(\"src\"): PROJECT_ROOT = os.getcwd()\n",
                "elif os.path.exists(\"llm-semeval-task8\"): PROJECT_ROOT = \"llm-semeval-task8\"\n",
                "else: PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "from src.generation import create_generation_components\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "print(f\"Project: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mode: FULL\n",
                        "Max docs/domain: 25000\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# --- IMPORTANT: Adjust these for your hardware ---\n",
                "# TEST_MODE = True uses small subsets for quick testing\n",
                "# TEST_MODE = False uses MAX_DOCS_PER_DOMAIN for submission\n",
                "TEST_MODE = False\n",
                "\n",
                "# For TEST mode (quick validation)\n",
                "TEST_CHUNK_LIMIT = 1000      # Chunks per domain for indexing\n",
                "TEST_QUERY_LIMIT = 5         # Conversations per domain\n",
                "\n",
                "# For FULL mode (final submission)\n",
                "# ~25,000 docs per domain = ~100k total = reasonable indexing time (~2.5 hours)\n",
                "MAX_DOCS_PER_DOMAIN = 25000  # Max documents to load per domain\n",
                "\n",
                "# Paths\n",
                "CORPUS_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONV_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "\n",
                "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
                "if not TEST_MODE:\n",
                "    print(f\"Max docs/domain: {MAX_DOCS_PER_DOMAIN}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(msgs): return next((m[\"text\"] for m in reversed(msgs) if m.get(\"speaker\")==\"user\"), \"\")\n",
                "\n",
                "def get_corpus(domain):\n",
                "    p = os.path.join(CORPUS_DIR, f\"{domain}.jsonl\")\n",
                "    z = p + \".zip\"\n",
                "    if not os.path.exists(p) and os.path.exists(z):\n",
                "        with zipfile.ZipFile(z) as zf: zf.extractall(CORPUS_DIR)\n",
                "    return p if os.path.exists(p) else None\n",
                "\n",
                "def save_jsonl(data, path):\n",
                "    with open(path, 'w') as f:\n",
                "        for d in data: f.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
                "    print(f\"Saved {len(data)} -> {path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "build_index",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Building 'mtrag_unified'...\n",
                        "Loading govt...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/govt.jsonl ---\n",
                        "Loaded 49607 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "  Limiting: 330681 -> 25000\n",
                        "  Added 25000 chunks\n",
                        "Loading clapnq...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/clapnq.jsonl ---\n",
                        "Loaded 183408 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "  Limiting: 530748 -> 25000\n",
                        "  Added 25000 chunks\n",
                        "Loading fiqa...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/fiqa.jsonl ---\n",
                        "Loaded 60984 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "  Limiting: 161865 -> 25000\n",
                        "  Added 25000 chunks\n",
                        "Loading cloud...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/cloud.jsonl ---\n",
                        "Loaded 72439 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "  Limiting: 435182 -> 25000\n",
                        "  Added 25000 chunks\n",
                        "Total: 100000 chunks\n",
                        "--- BUILDING VECTOR STORE: mtrag_unified ---\n",
                        "   Adding 100000 documents in batches of 64...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Indexing:  20%|â–ˆâ–‰        | 312/1563 [37:43<2:43:06,  7.82s/it]/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/langchain_qdrant/qdrant.py:513: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 20032 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
                        "  self.client.upsert(\n",
                        "Indexing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [3:30:37<00:00,  8.09s/it]  "
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- VECTOR STORE BUILT AND SAVED ---\n",
                        "Done.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Build Index\n",
                "need_build = True\n",
                "try:\n",
                "    client = get_qdrant_client(QDRANT_PATH)\n",
                "    if client.collection_exists(COLLECTION_NAME):\n",
                "        print(f\"Collection exists: {client.get_collection(COLLECTION_NAME).points_count} vectors\")\n",
                "        need_build = False\n",
                "except: pass\n",
                "\n",
                "if need_build:\n",
                "    print(f\"Building '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    limit = TEST_CHUNK_LIMIT if TEST_MODE else MAX_DOCS_PER_DOMAIN\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        path = get_corpus(domain)\n",
                "        if not path: continue\n",
                "        print(f\"Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(path)\n",
                "        for d in docs: d.metadata[\"domain\"] = domain\n",
                "        \n",
                "        # Apply limit\n",
                "        if len(docs) > limit:\n",
                "            print(f\"  Limiting: {len(docs)} -> {limit}\")\n",
                "            docs = docs[:limit]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"  Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"Total: {len(all_docs)} chunks\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"Done.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "init_components",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Retriever...\n",
                        "ðŸ”§ Loading reranker: BAAI/bge-reranker-v2-m3\n",
                        "Loading LLM...\n",
                        "Creating Generation Components with model: meta-llama/Llama-3.1-8B-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.15s/it]\n",
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generation Components Ready.\n",
                        "Ready.\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Components\n",
                "print(\"Loading Retriever...\")\n",
                "retriever = get_retriever(qdrant_path=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "\n",
                "print(\"Loading LLM...\")\n",
                "gen = create_generation_components()\n",
                "\n",
                "# Task B chain (no context)\n",
                "task_b_prompt = PromptTemplate(\n",
                "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are an expert assistant. Answer based on your knowledge. Be concise.<|eot_id|>\n",
                "<|start_header_id|>user<|end_header_id|>\n",
                "{question}<|eot_id|>\n",
                "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
                "    input_variables=[\"question\"]\n",
                ")\n",
                "task_b_chain = task_b_prompt | gen.llm | StrOutputParser()\n",
                "print(\"Ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "execute",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== GOVT ===\n",
                        "Processing 28 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "govt:  18%|â–ˆâ–Š        | 5/28 [00:48<03:23,  8.83s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
                        "govt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [04:50<00:00, 10.36s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== CLAPNQ ===\n",
                        "Processing 29 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "clapnq: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [06:39<00:00, 13.79s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== FIQA ===\n",
                        "Processing 27 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "fiqa: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [05:58<00:00, 13.28s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== CLOUD ===\n",
                        "Processing 26 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "cloud: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [05:47<00:00, 13.36s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Total: 110 results\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Execute Pipeline\n",
                "with open(CONV_FILE) as f: conversations = json.load(f)\n",
                "results_A, results_B, results_C = [], [], []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n=== {domain.upper()} ===\")\n",
                "    convs = [c for c in conversations if domain in c.get(\"domain\", \"\").lower()]\n",
                "    if TEST_MODE: convs = convs[:TEST_QUERY_LIMIT]\n",
                "    print(f\"Processing {len(convs)} conversations\")\n",
                "    \n",
                "    for conv in tqdm(convs, desc=domain):\n",
                "        msgs = conv.get(\"messages\", [])\n",
                "        q = extract_last_query(msgs)\n",
                "        if not q: continue\n",
                "        \n",
                "        # Task A: Retrieve\n",
                "        docs = retriever.invoke(q)\n",
                "        contexts = []\n",
                "        ctx_text = \"\"\n",
                "        for i, d in enumerate(docs):\n",
                "            txt = d.metadata.get(\"parent_text\") or d.page_content\n",
                "            contexts.append({\"document_id\": str(d.metadata.get(\"doc_id\", f\"{domain}_{i}\")), \"score\": 0.0, \"text\": txt})\n",
                "            ctx_text += f\"[{i+1}] {txt}\\n\\n\"\n",
                "        \n",
                "        # Task B: Generate (no context)\n",
                "        try: ans_b = task_b_chain.invoke({\"question\": q})\n",
                "        except Exception as e: ans_b = str(e)\n",
                "        \n",
                "        # Task C: RAG Generate\n",
                "        try: ans_c = gen.generator.invoke({\"context\": ctx_text, \"question\": q})\n",
                "        except Exception as e: ans_c = str(e)\n",
                "        \n",
                "        base = {\"conversation_id\": conv.get(\"author\"), \"task_id\": f\"{conv.get('author')}::1\", \"Collection\": f\"mt-rag-{domain}\", \"input\": msgs}\n",
                "        results_A.append({**base, \"contexts\": contexts})\n",
                "        results_B.append({**base, \"predictions\": [{\"text\": ans_b}]})\n",
                "        results_C.append({**base, \"contexts\": contexts, \"predictions\": [{\"text\": ans_c}]})\n",
                "\n",
                "print(f\"\\nTotal: {len(results_A)} results\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "save",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved 110 -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskA_Gbgers.jsonl\n",
                        "Saved 110 -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskB_Gbgers.jsonl\n",
                        "Saved 110 -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskC_Gbgers.jsonl\n",
                        "Done!\n"
                    ]
                }
            ],
            "source": [
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)\n",
                "print(\"Done!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
