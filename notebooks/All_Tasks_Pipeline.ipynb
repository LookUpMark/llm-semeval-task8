{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Unified Pipeline (All Tasks)\n",
                "\n",
                "This notebook executes the complete RAG pipeline once and generates submission files for **all three tasks** simultaneously.\n",
                "\n",
                "**Efficiency**: Instead of running the pipeline 3 times (which is slow & costly), we run it once per question and extract:\n",
                "- **Task A (Retrieval)**: The retrieved documents (`contexts`).\n",
                "- **Task B (Generation)**: The generated answer (`predictions`).\n",
                "- **Task C (RAG)**: Both contexts and predictions.\n",
                "\n",
                "## Output Files\n",
                "Files will be saved in `data/submissions/`:\n",
                "1. `submission_TaskA_YOURTEAM.jsonl`\n",
                "2. `submission_TaskB_YOURTEAM.jsonl`\n",
                "3. `submission_TaskC_YOURTEAM.jsonl`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- KAGGLE SETUP ---\n",
                "# Uncomment and run this cell FIRST if you are running on Kaggle.\n",
                "# It clones the repo, installs dependencies, and sets the working directory.\n",
                "\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -c scripts/evaluation/constraints.txt -r scripts/evaluation/requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import sys\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Locate Project Root (Robust for Local vs Kaggle)\n",
                "if os.path.exists(\"src\"):\n",
                "    project_root = os.getcwd()\n",
                "else:\n",
                "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "# Import RAG pipeline\n",
                "try:\n",
                "    from src.graph import app\n",
                "except ImportError:\n",
                "    print(\"ERROR: Run this notebook from the 'notebooks/' directory or repo root.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "BASE_PATH = \".\" if os.path.exists(\"dataset\") else \"..\"\n",
                "INPUT_FILE = os.path.join(BASE_PATH, \"dataset/human/generation_tasks/reference.jsonl\")\n",
                "SUBMISSION_DIR = os.path.join(BASE_PATH, \"data/submissions\")\n",
                "\n",
                "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
                "\n",
                "FILE_A = os.path.join(SUBMISSION_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(SUBMISSION_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(SUBMISSION_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "print(f\"Inputs: {INPUT_FILE}\")\n",
                "print(f\"Outputs:\\n 1. {FILE_A}\\n 2. {FILE_B}\\n 3. {FILE_C}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(path):\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        return [json.loads(line) for line in f if line.strip()]\n",
                "\n",
                "test_data = load_data(INPUT_FILE)\n",
                "print(f\"Loaded {len(test_data)} questions to process.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- EXECUTION ---\n",
                "results_A = []\n",
                "results_B = []\n",
                "results_C = []\n",
                "\n",
                "print(\"Starting Unified Pipeline Execution...\")\n",
                "\n",
                "for item in tqdm(test_data):\n",
                "    question = item.get(\"question\")\n",
                "    \n",
                "    # 1. RUN PIPELINE ONCE\n",
                "    try:\n",
                "        resp = app.invoke({\"question\": question})\n",
                "        raw_docs = resp.get(\"documents\", [])\n",
                "        gen_text = resp.get(\"generation\", \"\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error on '{question}': {e}\")\n",
                "        raw_docs = []\n",
                "        gen_text = \"Error\"\n",
                "        \n",
                "    # 2. FORMAT DATA\n",
                "    # Format Contexts for A and C\n",
                "    contexts = []\n",
                "    for doc in raw_docs:\n",
                "        meta = getattr(doc, \"metadata\", {})\n",
                "        contexts.append({\n",
                "            \"document_id\": meta.get(\"id\", meta.get(\"document_id\", \"unknown\")),\n",
                "            \"text\": getattr(doc, \"page_content\", \"\"),\n",
                "            \"score\": float(meta.get(\"relevance_score\", meta.get(\"score\", 0.0)))\n",
                "        })\n",
                "        \n",
                "    # Format Predictions for B and C\n",
                "    predictions = [{\"text\": gen_text if gen_text else \"No Answer\"}]\n",
                "    \n",
                "    # 3. BUILD RESULT OBJECTS\n",
                "    # Task A: contexts only\n",
                "    item_A = item.copy()\n",
                "    item_A[\"contexts\"] = contexts\n",
                "    results_A.append(item_A)\n",
                "    \n",
                "    # Task B: predictions only\n",
                "    item_B = item.copy()\n",
                "    item_B[\"predictions\"] = predictions\n",
                "    results_B.append(item_B)\n",
                "    \n",
                "    # Task C: contexts + predictions\n",
                "    item_C = item.copy()\n",
                "    item_C[\"contexts\"] = contexts\n",
                "    item_C[\"predictions\"] = predictions\n",
                "    results_C.append(item_C)\n",
                "\n",
                "print(\"Execution Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SAVE OUTPUTS ---\n",
                "\n",
                "def save_jsonl(data, path):\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for x in data:\n",
                "            json.dump(x, f)\n",
                "            f.write('\\n')\n",
                "    print(f\"Saved: {path} ({len(data)} items)\")\n",
                "\n",
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- VALIDATION ---\n",
                "print(\"Validating outputs...\")\n",
                "\n",
                "def check(data, task):\n",
                "    if not data: return False\n",
                "    s = data[0]\n",
                "    if task == \"A\": return \"contexts\" in s\n",
                "    if task == \"B\": return \"predictions\" in s\n",
                "    if task == \"C\": return \"contexts\" in s and \"predictions\" in s\n",
                "    return False\n",
                "\n",
                "print(f\"Task A Valid: {check(results_A, 'A')}\")\n",
                "print(f\"Task B Valid: {check(results_B, 'B')}\")\n",
                "print(f\"Task C Valid: {check(results_C, 'C')}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}