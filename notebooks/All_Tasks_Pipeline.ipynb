{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
    "\n",
    "## Complete Pipeline for All Tasks (Graph-Enhanced)\n",
    "\n",
    "This pipeline generates submissions for Tasks A, B, and C.\n",
    "\n",
    "| Task | Description | Method |\n",
    "|------|-------------|--------|\n",
    "| **A** | Retrieval | BGE-M3 + Cross-Encoder Reranking |\n",
    "| **B** | Generation | Direct LLM (Llama 3.1) |\n",
    "| **C** | RAG | Self-CRAG Graph with Hallucination Check |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_imports",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Project Root Detection ---\n",
    "if os.path.exists(\"src\"): PROJECT_ROOT = os.getcwd()\n",
    "elif os.path.exists(\"llm-semeval-task8\"): PROJECT_ROOT = \"llm-semeval-task8\"\n",
    "else: PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# --- Ingestion & Retrieval ---\n",
    "from src.ingestion import load_and_chunk_data, build_vector_store\n",
    "from src.retrieval import get_retriever, get_qdrant_client\n",
    "\n",
    "# --- Task B (Simple Gen) ---\n",
    "from src.generation import create_generation_components\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# --- Task C (Advanced Graph) ---\n",
    "from src.graph import initialize_graph\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_config",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Mode: FULL\n",
      "Max docs/domain: 25000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "TEAM_NAME = \"Gbgers\"\n",
    "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
    "COLLECTION_NAME = \"mtrag_unified\"\n",
    "\n",
    "# --- Execution Mode ---\n",
    "# TEST_MODE = True  -> Fast validation (~5 min)\n",
    "# TEST_MODE = False -> Full submission (~3-5 hours)\n",
    "TEST_MODE = False\n",
    "\n",
    "# --- Test Mode Limits ---\n",
    "TEST_CHUNK_LIMIT = 1000      # Chunks per domain for indexing\n",
    "TEST_QUERY_LIMIT = 5         # Conversations per domain\n",
    "\n",
    "# --- Full Mode Limits ---\n",
    "MAX_DOCS_PER_DOMAIN = 25000  # Max documents to load per domain\n",
    "\n",
    "# --- Paths ---\n",
    "CORPUS_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
    "CONV_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
    "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
    "\n",
    "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
    "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
    "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Execution Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
    "if not TEST_MODE:\n",
    "    print(f\"Max docs/domain: {MAX_DOCS_PER_DOMAIN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_helpers",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_query(msgs):\n",
    "    \"\"\"Extract the most recent user query from a conversation.\"\"\"\n",
    "    return next((m[\"text\"] for m in reversed(msgs) if m.get(\"speaker\")==\"user\"), \"\")\n",
    "\n",
    "def get_corpus(domain):\n",
    "    \"\"\"Get or extract corpus file path.\"\"\"\n",
    "    p = os.path.join(CORPUS_DIR, f\"{domain}.jsonl\")\n",
    "    z = p + \".zip\"\n",
    "    if not os.path.exists(p) and os.path.exists(z):\n",
    "        print(f\"Extracting {domain}.jsonl...\")\n",
    "        with zipfile.ZipFile(z) as zf: zf.extractall(CORPUS_DIR)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    \"\"\"Save list of dicts to JSONL file.\"\"\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for d in data: f.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "    print(f\"Saved {len(data)} items -> {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_index",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Build Unified Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build_index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'mtrag_unified' found: 100000 vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/src/retrieval.py:28: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Collection <mtrag_unified> contains 100000 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n",
      "  _qdrant_client = QdrantClient(path=qdrant_path)\n"
     ]
    }
   ],
   "source": [
    "# --- Check if Index Exists ---\n",
    "need_build = True\n",
    "try:\n",
    "    client = get_qdrant_client(QDRANT_PATH)\n",
    "    if client.collection_exists(COLLECTION_NAME):\n",
    "        info = client.get_collection(COLLECTION_NAME)\n",
    "        print(f\"Collection '{COLLECTION_NAME}' found: {info.points_count} vectors\")\n",
    "        need_build = False\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# --- Build if Needed ---\n",
    "if need_build:\n",
    "    print(f\"Building '{COLLECTION_NAME}'...\")\n",
    "    all_docs = []\n",
    "    limit = TEST_CHUNK_LIMIT if TEST_MODE else MAX_DOCS_PER_DOMAIN\n",
    "    \n",
    "    for domain in DOMAINS:\n",
    "        path = get_corpus(domain)\n",
    "        if not path:\n",
    "            print(f\"Warning: Corpus not found for {domain}\")\n",
    "            continue\n",
    "        print(f\"Loading {domain}...\")\n",
    "        docs = load_and_chunk_data(path)\n",
    "        for d in docs: d.metadata[\"domain\"] = domain\n",
    "        \n",
    "        if len(docs) > limit:\n",
    "            print(f\"  Limiting: {len(docs)} -> {limit}\")\n",
    "            docs = docs[:limit]\n",
    "        \n",
    "        all_docs.extend(docs)\n",
    "        print(f\"  Added {len(docs)} chunks\")\n",
    "    \n",
    "    print(f\"Total: {len(all_docs)} chunks\")\n",
    "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
    "    print(\"Index built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_init",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Initialize Components (Retriever, LLM, Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "init_components",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Retriever...\n",
      "Loading reranker: BAAI/bge-reranker-v2-m3\n",
      "Loading LLM for Task B...\n",
      "Creating Generation Components with model: meta-llama/Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.52s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation Components Ready.\n",
      "Initializing Advanced Graph for Task C...\n",
      "All Systems Ready.\n"
     ]
    }
   ],
   "source": [
    "# --- 5.1 Initialize Retriever ---\n",
    "print(\"Loading Retriever...\")\n",
    "retriever = get_retriever(qdrant_path=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# --- 5.2 Initialize LLM for Task B ---\n",
    "print(\"Loading LLM for Task B...\")\n",
    "gen_components = create_generation_components()\n",
    "\n",
    "# --- 5.3 Initialize Self-CRAG Graph for Task C ---\n",
    "print(\"Initializing Advanced Graph for Task C...\")\n",
    "# CRITICAL FIX: Share the SAME generation components with the graph to avoid OOM\n",
    "# Otherwise, the graph would load a SECOND copy of the model.\n",
    "import src.graph\n",
    "src.graph._components = gen_components\n",
    "graph_app = initialize_graph()\n",
    "\n",
    "# --- 5.4 Create Task B Chain (no context) ---\n",
    "task_b_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert assistant. Answer based on your knowledge. Be concise.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "task_b_chain = task_b_prompt | gen_components.llm | StrOutputParser()\n",
    "\n",
    "print(\"All Systems Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_execute",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Execute Unified Pipeline\n",
    "\n",
    "This cell processes all conversations and generates results for all three tasks.\n",
    "\n",
    "**Note on `conversation_id`:** The dataset does NOT have a native `conversation_id` key. \n",
    "We generate a unique ID using `{domain}_{index}` format (e.g., `govt_0`, `govt_1`, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conversations...\n",
      "Total conversations: 110\n",
      "\n",
      "JSON Keys: ['author', 'retriever', 'generator', 'messages', 'status', 'status_history', 'editor', 'domain', 'reviewer']\n",
      "\n",
      "==================================================\n",
      " DOMAIN: GOVT\n",
      "==================================================\n",
      "Processing 28 conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "govt:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever for domain: govt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "govt:   4%|▎         | 1/28 [00:28<13:01, 28.96s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "govt: 100%|██████████| 28/28 [08:55<00:00, 19.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " DOMAIN: CLAPNQ\n",
      "==================================================\n",
      "Processing 29 conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clapnq:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever for domain: clapnq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clapnq: 100%|██████████| 29/29 [08:48<00:00, 18.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " DOMAIN: FIQA\n",
      "==================================================\n",
      "Processing 27 conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fiqa:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever for domain: fiqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fiqa: 100%|██████████| 27/27 [08:53<00:00, 19.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " DOMAIN: CLOUD\n",
      "==================================================\n",
      "Processing 26 conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cloud:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever for domain: cloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cloud: 100%|██████████| 26/26 [08:48<00:00, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete. Total Results: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Load Conversations ---\n",
    "print(\"Loading conversations...\")\n",
    "with open(CONV_FILE) as f:\n",
    "    conversations = json.load(f)\n",
    "print(f\"Total conversations: {len(conversations)}\")\n",
    "\n",
    "# --- Analyze JSON structure (first run info) ---\n",
    "print(f\"\\nJSON Keys: {list(conversations[0].keys())}\")\n",
    "\n",
    "# --- Initialize Result Containers ---\n",
    "results_A, results_B, results_C = [], [], []\n",
    "\n",
    "# --- Process Each Domain ---\n",
    "for domain in DOMAINS:\n",
    "    print(f\"\\n{'='*50}\\n DOMAIN: {domain.upper()}\\n{'='*50}\")\n",
    "    \n",
    "    # Filter by domain (substring match on the 'domain' key), keeping track of global index\n",
    "    convs_with_idx = []\n",
    "    for i, c in enumerate(conversations):\n",
    "        if domain in c.get(\"domain\", \"\").lower():\n",
    "            convs_with_idx.append((i, c))\n",
    "            \n",
    "    if TEST_MODE:\n",
    "        convs_with_idx = convs_with_idx[:TEST_QUERY_LIMIT]\n",
    "    print(f\"Processing {len(convs_with_idx)} conversations\")\n",
    "    \n",
    "    for idx_in_domain, (global_idx, conv) in enumerate(tqdm(convs_with_idx, desc=domain)):\n",
    "        msgs = conv.get(\"messages\", [])\n",
    "        q = extract_last_query(msgs)\n",
    "        if not q:\n",
    "            continue\n",
    "        \n",
    "        # --- Generate unique conversation_id ---\n",
    "        # Format: {domain}_{index_within_domain}\n",
    "        conv_id = f\"{domain}_{idx_in_domain}\"\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # TASK A: Retrieval\n",
    "        # -------------------------------------------------------------------\n",
    "        docs = retriever.invoke(q)\n",
    "        contexts = []\n",
    "        for i, d in enumerate(docs):\n",
    "            txt = d.metadata.get(\"parent_text\") or d.page_content\n",
    "            contexts.append({\n",
    "                \"document_id\": str(d.metadata.get(\"doc_id\", f\"{domain}_{i}\")),\n",
    "                \"score\": float(d.metadata.get(\"relevance_score\", 0.0)),\n",
    "                \"text\": txt\n",
    "            })\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # TASK B: Generation (Direct LLM, No Context)\n",
    "        # -------------------------------------------------------------------\n",
    "        try:\n",
    "            ans_b = task_b_chain.invoke({\"question\": q})\n",
    "        except Exception as e:\n",
    "            ans_b = str(e)\n",
    "        \n",
    "        # -------------------------------------------------------------------\n",
    "        # TASK C: RAG Generation (Self-CRAG Graph)\n",
    "        # -------------------------------------------------------------------\n",
    "        try:\n",
    "            # Convert raw msgs to LangChain objects for usage in Rewrite Node\n",
    "            chat_history = []\n",
    "            for m in msgs:\n",
    "                role = m.get(\"speaker\", \"user\")\n",
    "                content = m.get(\"text\", \"\")\n",
    "                if role == \"user\":\n",
    "                    chat_history.append(HumanMessage(content=content))\n",
    "                else:\n",
    "                    chat_history.append(AIMessage(content=content))\n",
    "            \n",
    "            # Pass messages to graph for rewriter\n",
    "            response = graph_app.invoke({\n",
    "                \"question\": q, \n",
    "                \"domain\": domain,\n",
    "                \"messages\": chat_history\n",
    "            })\n",
    "            ans_c = response.get(\"generation\", \"I_DONT_KNOW\")\n",
    "            reason_c = response.get(\"fallback_reason\", \"none\")\n",
    "            docs_relevant_c = response.get(\"documents_relevant\", \"unknown\")\n",
    "            is_hallucination_c = response.get(\"is_hallucination\", \"unknown\")\n",
    "            retry_count_c = response.get(\"retry_count\", 0)\n",
    "        except Exception as e:\n",
    "            print(f\"Task C Error: {e}\")\n",
    "            ans_c = \"I_DONT_KNOW\"\n",
    "            \n",
    "        # -------------------------------------------------------------------\n",
    "        # Append Results\n",
    "        # -------------------------------------------------------------------\n",
    "        results_A.append({\"conversation_id\": conv_id, \"original_index\": global_idx, \"ranking\": contexts})\n",
    "        results_B.append({\"conversation_id\": conv_id, \"original_index\": global_idx, \"answer\": ans_b})\n",
    "        results_C.append({\"conversation_id\": conv_id, \"original_index\": global_idx, \"answer\": ans_c, \"fallback_reason\": reason_c, \"docs_relevant\": docs_relevant_c, \"is_hallucination\": is_hallucination_c, \"retry_count\": retry_count_c, \"references\": contexts})\n",
    "        \n",
    "print(f\"\\nProcessing Complete. Total Results: {len(results_A)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving submission files...\n",
      "Saved 110 items -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskA_Gbgers.jsonl\n",
      "Saved 110 items -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskB_Gbgers.jsonl\n",
      "Saved 110 items -> /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskC_Gbgers.jsonl\n",
      "\n",
      "All Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving submission files...\")\n",
    "\n",
    "save_jsonl(results_A, FILE_A)\n",
    "save_jsonl(results_B, FILE_B)\n",
    "save_jsonl(results_C, FILE_C)\n",
    "\n",
    "print(\"\\nAll Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
