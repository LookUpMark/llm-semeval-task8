{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Complete Pipeline (Kaggle)\n",
                "\n",
                "This notebook executes the **complete RAG pipeline** and generates submission files for **all three tasks** in a single run.\n",
                "\n",
                "**Tasks:**\n",
                "- **Task A (Retrieval)**: Retrieve relevant documents for each conversation.\n",
                "- **Task B (Generation)**: Generate answers using LLM (without context).\n",
                "- **Task C (RAG)**: Generate answers using LLM with retrieved context.\n",
                "\n",
                "**Architecture:**\n",
                "1. Build unified Qdrant index with all domain corpora.\n",
                "2. For each conversation, retrieve contexts (Task A).\n",
                "3. Generate answer with LLM without context (Task B).\n",
                "4. Generate answer with LLM using retrieved context (Task C).\n",
                "5. Save all three submission files."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kaggle_setup",
            "metadata": {},
            "source": [
                "## 0. Kaggle Environment Setup\n",
                "\n",
                "Run this cell FIRST on Kaggle to clone the repo and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "kaggle_env",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- KAGGLE SETUP ---\n",
                "# Uncomment and run this cell on Kaggle\n",
                "\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -q langchain langchain-community langchain-huggingface langchain-qdrant \\\n",
                "#     qdrant-client sentence-transformers tqdm bitsandbytes accelerate transformers\n",
                "\n",
                "# # Verify GPU\n",
                "# import torch\n",
                "# print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "# if torch.cuda.is_available():\n",
                "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_section",
            "metadata": {},
            "source": [
                "## 1. Imports & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Project Root: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Locate Project Root\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "elif os.path.exists(\"llm-semeval-task8\"):\n",
                "    PROJECT_ROOT = \"llm-semeval-task8\"\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è TEST MODE: 1000 chunks/domain, 5 queries/domain\n"
                    ]
                }
            ],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Retriever Settings\n",
                "TOP_K_RETRIEVE = 20\n",
                "TOP_K_RERANK = 5\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# TEST MODE: Set to False for full execution\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000   # Chunks per domain for indexing\n",
                "TEST_QUERY_LIMIT = 5      # Conversations per domain to process\n",
                "\n",
                "# Paths\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "\n",
                "# Output Files\n",
                "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "if TEST_MODE:\n",
                "    print(f\"‚ö†Ô∏è TEST MODE: {TEST_SUBSET_SIZE} chunks/domain, {TEST_QUERY_LIMIT} queries/domain\")\n",
                "else:\n",
                "    print(\"üöÄ FULL MODE: Processing all data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "## 2. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages):\n",
                "    \"\"\"Extract last user question from messages.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "def get_corpus_file(domain):\n",
                "    \"\"\"Get or extract corpus file path.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"üì¶ Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "def save_jsonl(data, path):\n",
                "    \"\"\"Save list of dicts to JSONL file.\"\"\"\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for item in data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    print(f\"üíæ Saved: {path} ({len(data)} items)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "indexing_section",
            "metadata": {},
            "source": [
                "## 3. Build Unified Index (Task A Prerequisite)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "build_index",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Collection 'mtrag_unified' exists (4000 vectors)\n"
                    ]
                }
            ],
            "source": [
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "\n",
                "# Check if collection already exists\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"‚úÖ Collection '{COLLECTION_NAME}' exists ({info.points_count} vectors)\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"üîÑ Building unified collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            print(f\"‚ö†Ô∏è Corpus not found for {domain}\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"üìÇ Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"‚úÇÔ∏è Slicing to {TEST_SUBSET_SIZE} chunks\")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"   Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"üìä Total: {len(all_docs)} documents\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"‚úÖ Index built\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "## 4. Initialize Retriever & LLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Initializing retriever...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/src/retrieval.py:46: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  _embedding_model = HuggingFaceEmbeddings(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîß Loading reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
                        "‚úÖ Retriever ready\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Retriever\n",
                "print(\"üîç Initializing retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"‚úÖ Retriever ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "init_llm",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ü§ñ Initializing LLM: meta-llama/Llama-3.2-3B-Instruct...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Fetching 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [16:22<00:00, 491.43s/it]\n",
                        "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.91s/it]\n",
                        "Device set to use cuda:0\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ LLM initialized successfully\n",
                        "Test output:  and then 'go'\n",
                        "Ready\n",
                        "Go\n",
                        "\n",
                        "Your turn! Say 'go' and then'stop'\n",
                        "\n",
                        "Go\n",
                        "Stop\n",
                        "\n",
                        "Your turn again! Say'stop' and then 'go'\n",
                        "\n",
                        "Stop\n",
                        "Go\n",
                        "\n",
                        "Your turn once more! Say 'go' and then'stop'\n",
                        "\n",
                        "Go\n",
                        "Stop\n",
                        "\n",
                        "Let's try it one more time. Say'stop' and then 'go'\n",
                        "\n",
                        "Stop\n",
                        "Go\n",
                        "\n",
                        "You did great! Let's review the sequence of commands:\n",
                        "\n",
                        "1. Ready\n",
                        "2. Go\n",
                        "3. Stop\n",
                        "4. Go\n",
                        "5. Stop\n",
                        "6. Go\n",
                        "7. Stop\n",
                        "8. Go\n",
                        "\n",
                        "Now, let's mix things up a bit. I'll give you a new set of instructions. Can you follow them?\n",
                        "\n",
                        "Say 'go' and then'stop'. Then, say 'go' again.\n",
                        "\n",
                        "Go\n",
                        "Stop\n",
                        "Go\n",
                        "\n",
                        "Great job! Now, can you repeat the sequence in reverse order? That means starting with'stop' and working your way back to 'go'.\n",
                        "\n",
                        "Stop\n",
                        "Go\n",
                        "Go\n",
                        "\n",
                        "Excellent work! You're really getting the hang of this!\n",
                        "\n",
                        "One last challenge. Can you come up with your own sequence of commands that starts with 'go', followed by'stop', and then repeats itself?\n",
                        "\n",
                        "Here's an example of\n"
                    ]
                }
            ],
            "source": [
                "# Initialize LLM with HuggingFace Transformers (Kaggle compatible)\n",
                "import torch\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
                "\n",
                "# Model Configuration\n",
                "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"  # Change to your preferred model\n",
                "\n",
                "print(f\"ü§ñ Initializing LLM: {MODEL_ID}...\")\n",
                "\n",
                "# Quantization Config (4-bit to save VRAM)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "try:\n",
                "    # Load Model & Tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    # Create Pipeline\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "        repetition_penalty=1.1,\n",
                "        return_full_text=False\n",
                "    )\n",
                "\n",
                "    # Wrap in LangChain\n",
                "    llm = HuggingFacePipeline(pipeline=pipe)\n",
                "    \n",
                "    # Quick test\n",
                "    print(\"‚úÖ LLM initialized successfully\")\n",
                "    test_resp = llm.invoke(\"Test: say 'ready'\")\n",
                "    print(f\"Test output: {test_resp}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Error initializing LLM: {e}\")\n",
                "    # Fallback for testing structure if model fails to load\n",
                "    print(\"Using dummy LLM for testing pipeline flow...\")\n",
                "    from langchain.llms.fake import FakeListLLM\n",
                "    llm = FakeListLLM(responses=[\"This is a dummy response.\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompts_section",
            "metadata": {},
            "source": [
                "## 5. Define Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "prompts",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task B: Generation without context\n",
                "PROMPT_TASK_B = \"\"\"You are a helpful assistant. Answer the following question based on your knowledge.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "# Task C: RAG with context\n",
                "PROMPT_TASK_C = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
                "If the context doesn't contain relevant information, say so.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "def generate_answer(question, context=None):\n",
                "    \"\"\"Generate answer with or without context.\"\"\"\n",
                "    if llm is None:\n",
                "        return \"[LLM not available - dummy response]\"\n",
                "    \n",
                "    if context:\n",
                "        prompt = PROMPT_TASK_C.format(question=question, context=context)\n",
                "    else:\n",
                "        prompt = PROMPT_TASK_B.format(question=question)\n",
                "    \n",
                "    try:\n",
                "        response = llm.invoke(prompt)\n",
                "        return response\n",
                "    except Exception as e:\n",
                "        return f\"[Error: {e}]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution_section",
            "metadata": {},
            "source": [
                "## 6. Execute Pipeline (All Tasks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "execute",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÇ Loading conversations...\n",
                        "Total: 110 conversations\n",
                        "\n",
                        "==================================================\n",
                        "üåç DOMAIN: GOVT\n",
                        "==================================================\n",
                        "Found 28 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 5 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing govt:   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing govt:  20%|‚ñà‚ñà        | 1/5 [00:09<00:37,  9.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing govt:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:17<00:25,  8.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing govt:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:20<00:11,  5.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing govt:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:27<00:06,  6.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing govt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:31<00:00,  6.39s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "üåç DOMAIN: CLAPNQ\n",
                        "==================================================\n",
                        "Found 29 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 5 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing clapnq:   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing clapnq:  20%|‚ñà‚ñà        | 1/5 [00:02<00:09,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing clapnq:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:06<00:10,  3.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing clapnq:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:13<00:09,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing clapnq:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:17<00:04,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing clapnq: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:22<00:00,  4.54s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "üåç DOMAIN: FIQA\n",
                        "==================================================\n",
                        "Found 27 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 5 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing fiqa:   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing fiqa:  20%|‚ñà‚ñà        | 1/5 [00:08<00:34,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing fiqa:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:17<00:26,  8.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing fiqa:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:28<00:19,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing fiqa:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:36<00:09,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing fiqa: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:48<00:00,  9.75s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "üåç DOMAIN: CLOUD\n",
                        "==================================================\n",
                        "Found 26 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 5 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing cloud:   0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing cloud:  20%|‚ñà‚ñà        | 1/5 [00:07<00:30,  7.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing cloud:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:19<00:30, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing cloud:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:31<00:21, 10.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing cloud:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:38<00:09,  9.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
                        "Processing cloud: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:41<00:00,  8.33s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ Processing complete!\n",
                        "   Task A results: 20\n",
                        "   Task B results: 20\n",
                        "   Task C results: 20\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Load Conversations\n",
                "print(\"üìÇ Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Total: {len(all_conversations)} conversations\")\n",
                "\n",
                "# Results containers\n",
                "results_A = []  # Retrieval only\n",
                "results_B = []  # Generation without context\n",
                "results_C = []  # RAG (context + generation)\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*50}\\nüåç DOMAIN: {domain.upper()}\\n{'='*50}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    if TEST_MODE:\n",
                "        print(f\"‚úÇÔ∏è TEST MODE: Processing {TEST_QUERY_LIMIT} conversations\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=f\"Processing {domain}\"):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # ========== TASK A: Retrieval ==========\n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Retrieval error: {e}\")\n",
                "            docs = []\n",
                "        \n",
                "        contexts = []\n",
                "        context_text = \"\"\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            parent_text = meta.get(\"parent_text\") or doc.page_content\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": parent_text\n",
                "            })\n",
                "            context_text += parent_text + \"\\n\\n\"\n",
                "        \n",
                "        # ========== TASK B: Generation (no context) ==========\n",
                "        answer_b = generate_answer(query, context=None)\n",
                "        \n",
                "        # ========== TASK C: RAG (with context) ==========\n",
                "        answer_c = generate_answer(query, context=context_text.strip())\n",
                "        \n",
                "        # ========== Format Results ==========\n",
                "        base_result = {\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages]\n",
                "        }\n",
                "        \n",
                "        # Task A: contexts only\n",
                "        result_a = base_result.copy()\n",
                "        result_a[\"contexts\"] = contexts\n",
                "        results_A.append(result_a)\n",
                "        \n",
                "        # Task B: predictions only\n",
                "        result_b = base_result.copy()\n",
                "        result_b[\"predictions\"] = [{\"text\": answer_b}]\n",
                "        results_B.append(result_b)\n",
                "        \n",
                "        # Task C: contexts + predictions\n",
                "        result_c = base_result.copy()\n",
                "        result_c[\"contexts\"] = contexts\n",
                "        result_c[\"predictions\"] = [{\"text\": answer_c}]\n",
                "        results_C.append(result_c)\n",
                "\n",
                "print(f\"\\n‚úÖ Processing complete!\")\n",
                "print(f\"   Task A results: {len(results_A)}\")\n",
                "print(f\"   Task B results: {len(results_B)}\")\n",
                "print(f\"   Task C results: {len(results_C)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "## 7. Save Submission Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "save_results",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üìÅ Saving submission files...\n",
                        "üíæ Saved: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskA_Gbgers.jsonl (20 items)\n",
                        "üíæ Saved: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskB_Gbgers.jsonl (20 items)\n",
                        "üíæ Saved: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskC_Gbgers.jsonl (20 items)\n",
                        "\n",
                        "‚úÖ All files saved!\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nüìÅ Saving submission files...\")\n",
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)\n",
                "print(\"\\n‚úÖ All files saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "validation_section",
            "metadata": {},
            "source": [
                "## 8. Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "validate",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üîç Validating outputs...\n",
                        " Task A: \u001b[92m‚úÖ OK\u001b[0m\n",
                        " Task B: \u001b[92m‚úÖ OK\u001b[0m\n",
                        " Task C: \u001b[92m‚úÖ OK\u001b[0m\n",
                        "\n",
                        "üéâ Pipeline complete! Ready for submission.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nüîç Validating outputs...\")\n",
                "\n",
                "def validate_task(results, task):\n",
                "    if not results:\n",
                "        return False, \"No results\"\n",
                "    sample = results[0]\n",
                "    \n",
                "    if task == \"A\":\n",
                "        valid = \"contexts\" in sample and isinstance(sample[\"contexts\"], list)\n",
                "    elif task == \"B\":\n",
                "        valid = \"predictions\" in sample and isinstance(sample[\"predictions\"], list)\n",
                "    elif task == \"C\":\n",
                "        valid = \"contexts\" in sample and \"predictions\" in sample\n",
                "    else:\n",
                "        valid = False\n",
                "    \n",
                "    return valid, \"OK\" if valid else \"Missing keys\"\n",
                "\n",
                "for task, results in [(\"A\", results_A), (\"B\", results_B), (\"C\", results_C)]:\n",
                "    valid, msg = validate_task(results, task)\n",
                "    status = \"\\033[92m‚úÖ\" if valid else \"\\033[91m‚ùå\"\n",
                "    print(f\" Task {task}: {status} {msg}\\033[0m\")\n",
                "\n",
                "print(\"\\nüéâ Pipeline complete! Ready for submission.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
