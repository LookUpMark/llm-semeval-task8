{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Complete Pipeline (Kaggle)\n",
                "\n",
                "This notebook executes the **complete RAG pipeline** and generates submission files for **all three tasks** in a single run.\n",
                "\n",
                "**Tasks:**\n",
                "- **Task A (Retrieval)**: Retrieve relevant documents for each conversation.\n",
                "- **Task B (Generation)**: Generate answers using LLM (without context).\n",
                "- **Task C (RAG)**: Generate answers using LLM with retrieved context.\n",
                "\n",
                "**Architecture:**\n",
                "1. Build unified Qdrant index with all domain corpora.\n",
                "2. For each conversation, retrieve contexts (Task A).\n",
                "3. Generate answer with LLM without context (Task B).\n",
                "4. Generate answer with LLM using retrieved context (Task C).\n",
                "5. Save all three submission files."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kaggle_setup",
            "metadata": {},
            "source": [
                "## 0. Kaggle Environment Setup\n",
                "\n",
                "Run this cell FIRST on Kaggle to clone the repo and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kaggle_env",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- KAGGLE SETUP ---\n",
                "# Uncomment and run this cell on Kaggle\n",
                "\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -q langchain langchain-community langchain-huggingface langchain-qdrant \\\n",
                "#     qdrant-client sentence-transformers tqdm bitsandbytes accelerate transformers\n",
                "\n",
                "# # Verify GPU\n",
                "# import torch\n",
                "# print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "# if torch.cuda.is_available():\n",
                "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_section",
            "metadata": {},
            "source": [
                "## 1. Imports & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Locate Project Root\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "elif os.path.exists(\"llm-semeval-task8\"):\n",
                "    PROJECT_ROOT = \"llm-semeval-task8\"\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Retriever Settings\n",
                "TOP_K_RETRIEVE = 20\n",
                "TOP_K_RERANK = 5\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# TEST MODE: Set to False for full execution\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000   # Chunks per domain for indexing\n",
                "TEST_QUERY_LIMIT = 5      # Conversations per domain to process\n",
                "\n",
                "# Paths\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "\n",
                "# Output Files\n",
                "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "if TEST_MODE:\n",
                "    print(f\"‚ö†Ô∏è TEST MODE: {TEST_SUBSET_SIZE} chunks/domain, {TEST_QUERY_LIMIT} queries/domain\")\n",
                "else:\n",
                "    print(\"üöÄ FULL MODE: Processing all data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "## 2. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages):\n",
                "    \"\"\"Extract last user question from messages.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "def get_corpus_file(domain):\n",
                "    \"\"\"Get or extract corpus file path.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"üì¶ Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "def save_jsonl(data, path):\n",
                "    \"\"\"Save list of dicts to JSONL file.\"\"\"\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for item in data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    print(f\"üíæ Saved: {path} ({len(data)} items)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "indexing_section",
            "metadata": {},
            "source": [
                "## 3. Build Unified Index (Task A Prerequisite)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_index",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "\n",
                "# Check if collection already exists\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"‚úÖ Collection '{COLLECTION_NAME}' exists ({info.points_count} vectors)\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"üîÑ Building unified collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            print(f\"‚ö†Ô∏è Corpus not found for {domain}\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"üìÇ Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"‚úÇÔ∏è Slicing to {TEST_SUBSET_SIZE} chunks\")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"   Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"üìä Total: {len(all_docs)} documents\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"‚úÖ Index built\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "## 4. Initialize Retriever & LLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Retriever\n",
                "print(\"üîç Initializing retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"‚úÖ Retriever ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LLM with HuggingFace Transformers (Kaggle compatible)\n",
                "import torch\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
                "\n",
                "# Model Configuration\n",
                "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"  # Upgrade to 8B for better quality\n",
                "\n",
                "print(f\"ü§ñ Initializing LLM: {MODEL_ID}...\")\n",
                "\n",
                "# Quantization Config (4-bit to save VRAM - Fits in 8GB VRAM)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "try:\n",
                "    # Load Model & Tokenizer\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    # Create Pipeline\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "        repetition_penalty=1.1,\n",
                "        return_full_text=False\n",
                "    )\n",
                "\n",
                "    # Wrap in LangChain\n",
                "    llm = HuggingFacePipeline(pipeline=pipe)\n",
                "    \n",
                "    # Quick test\n",
                "    print(\"‚úÖ LLM initialized successfully\")\n",
                "    test_resp = llm.invoke(\"Test: say 'ready'\")\n",
                "    print(f\"Test output: {test_resp}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Error initializing LLM: {e}\")\n",
                "    # Fallback for testing structure if model fails to load\n",
                "    print(\"Using dummy LLM for testing pipeline flow...\")\n",
                "    from langchain.llms.fake import FakeListLLM\n",
                "    llm = FakeListLLM(responses=[\"This is a dummy response.\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompts_section",
            "metadata": {},
            "source": [
                "## 5. Define Prompts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompts",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Task B: Generation without context\n",
                "PROMPT_TASK_B = \"\"\"You are a helpful assistant. Answer the following question based on your knowledge.\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "# Task C: RAG with context\n",
                "PROMPT_TASK_C = \"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
                "If the context doesn't contain relevant information, say so.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "def generate_answer(question, context=None):\n",
                "    \"\"\"Generate answer with or without context.\"\"\"\n",
                "    if llm is None:\n",
                "        return \"[LLM not available - dummy response]\"\n",
                "    \n",
                "    if context:\n",
                "        prompt = PROMPT_TASK_C.format(question=question, context=context)\n",
                "    else:\n",
                "        prompt = PROMPT_TASK_B.format(question=question)\n",
                "    \n",
                "    try:\n",
                "        response = llm.invoke(prompt)\n",
                "        return response\n",
                "    except Exception as e:\n",
                "        return f\"[Error: {e}]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution_section",
            "metadata": {},
            "source": [
                "## 6. Execute Pipeline (All Tasks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Conversations\n",
                "print(\"üìÇ Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Total: {len(all_conversations)} conversations\")\n",
                "\n",
                "# Results containers\n",
                "results_A = []  # Retrieval only\n",
                "results_B = []  # Generation without context\n",
                "results_C = []  # RAG (context + generation)\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*50}\\nüåç DOMAIN: {domain.upper()}\\n{'='*50}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    if TEST_MODE:\n",
                "        print(f\"‚úÇÔ∏è TEST MODE: Processing {TEST_QUERY_LIMIT} conversations\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=f\"Processing {domain}\"):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # ========== TASK A: Retrieval ==========\n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Retrieval error: {e}\")\n",
                "            docs = []\n",
                "        \n",
                "        contexts = []\n",
                "        context_text = \"\"\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            parent_text = meta.get(\"parent_text\") or doc.page_content\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": parent_text\n",
                "            })\n",
                "            context_text += parent_text + \"\\n\\n\"\n",
                "        \n",
                "        # ========== TASK B: Generation (no context) ==========\n",
                "        answer_b = generate_answer(query, context=None)\n",
                "        \n",
                "        # ========== TASK C: RAG (with context) ==========\n",
                "        answer_c = generate_answer(query, context=context_text.strip())\n",
                "        \n",
                "        # ========== Format Results ==========\n",
                "        base_result = {\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages]\n",
                "        }\n",
                "        \n",
                "        # Task A: contexts only\n",
                "        result_a = base_result.copy()\n",
                "        result_a[\"contexts\"] = contexts\n",
                "        results_A.append(result_a)\n",
                "        \n",
                "        # Task B: predictions only\n",
                "        result_b = base_result.copy()\n",
                "        result_b[\"predictions\"] = [{\"text\": answer_b}]\n",
                "        results_B.append(result_b)\n",
                "        \n",
                "        # Task C: contexts + predictions\n",
                "        result_c = base_result.copy()\n",
                "        result_c[\"contexts\"] = contexts\n",
                "        result_c[\"predictions\"] = [{\"text\": answer_c}]\n",
                "        results_C.append(result_c)\n",
                "\n",
                "print(f\"\\n‚úÖ Processing complete!\")\n",
                "print(f\"   Task A results: {len(results_A)}\")\n",
                "print(f\"   Task B results: {len(results_B)}\")\n",
                "print(f\"   Task C results: {len(results_C)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "## 7. Save Submission Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüìÅ Saving submission files...\")\n",
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)\n",
                "print(\"\\n‚úÖ All files saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "validation_section",
            "metadata": {},
            "source": [
                "## 8. Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "validate",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nüîç Validating outputs...\")\n",
                "\n",
                "def validate_task(results, task):\n",
                "    if not results:\n",
                "        return False, \"No results\"\n",
                "    sample = results[0]\n",
                "    \n",
                "    if task == \"A\":\n",
                "        valid = \"contexts\" in sample and isinstance(sample[\"contexts\"], list)\n",
                "    elif task == \"B\":\n",
                "        valid = \"predictions\" in sample and isinstance(sample[\"predictions\"], list)\n",
                "    elif task == \"C\":\n",
                "        valid = \"contexts\" in sample and \"predictions\" in sample\n",
                "    else:\n",
                "        valid = False\n",
                "    \n",
                "    return valid, \"OK\" if valid else \"Missing keys\"\n",
                "\n",
                "for task, results in [(\"A\", results_A), (\"B\", results_B), (\"C\", results_C)]:\n",
                "    valid, msg = validate_task(results, task)\n",
                "    status = \"\\033[92m‚úÖ\" if valid else \"\\033[91m‚ùå\"\n",
                "    print(f\" Task {task}: {status} {msg}\\033[0m\")\n",
                "\n",
                "print(\"\\nüéâ Pipeline complete! Ready for submission.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}