{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 8: Multi-Turn RAG Pipeline\n",
    "\n",
    "**Team:** Gbgers  \n",
    "**Architecture:** Self-CRAG with LangGraph Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "| Task | Description | Method |\n",
    "|------|-------------|--------|\n",
    "| **A** | Retrieval | BGE-M3 → Cross-Encoder Reranking (Top-20 → Top-5) |\n",
    "| **B** | Generation | Direct LLM (Llama 3.1 8B, No Context) |\n",
    "| **C** | RAG | Self-CRAG Graph with Hallucination Check |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import os, sys, json, zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Project Root Detection ---\n",
    "if os.path.exists(\"src\"): PROJECT_ROOT = os.getcwd()\n",
    "elif os.path.exists(\"llm-semeval-task8\"): PROJECT_ROOT = \"llm-semeval-task8\"\n",
    "else: PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# --- Core Modules ---\n",
    "from src.ingestion import load_and_chunk_data, build_vector_store\n",
    "from src.retrieval import get_retriever, get_qdrant_client\n",
    "from src.generation import create_generation_components\n",
    "from src.graph import initialize_graph\n",
    "\n",
    "# --- LangChain ---\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: FULL | Max Docs/Domain: 25000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "TEAM_NAME = \"Gbgers\"\n",
    "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]  # 4 domain corpora\n",
    "COLLECTION_NAME = \"mtrag_unified\"              # Unified Qdrant collection\n",
    "\n",
    "# --- Execution Mode ---\n",
    "TEST_MODE = False  # False = Full submission (~3h)\n",
    "\n",
    "# --- Limits ---\n",
    "TEST_CHUNK_LIMIT = 1000   # Chunks/domain (test mode)\n",
    "TEST_QUERY_LIMIT = 5      # Conversations/domain (test mode)\n",
    "MAX_DOCS_PER_DOMAIN = 25000  # Full mode: 25k * 4 = 100k total\n",
    "\n",
    "# --- Paths ---\n",
    "CORPUS_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
    "CONV_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
    "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
    "\n",
    "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
    "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
    "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'} | Max Docs/Domain: {MAX_DOCS_PER_DOMAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_last_query(msgs):\n",
    "    \"\"\"Extract the most recent user query from conversation history.\"\"\"\n",
    "    return next((m[\"text\"] for m in reversed(msgs) if m.get(\"speaker\") == \"user\"), \"\")\n",
    "\n",
    "\n",
    "def get_corpus(domain):\n",
    "    \"\"\"Get or extract corpus file path (auto-unzip if needed).\"\"\"\n",
    "    p = os.path.join(CORPUS_DIR, f\"{domain}.jsonl\")\n",
    "    z = p + \".zip\"\n",
    "    if not os.path.exists(p) and os.path.exists(z):\n",
    "        print(f\"Extracting {domain}.jsonl...\")\n",
    "        with zipfile.ZipFile(z) as zf:\n",
    "            zf.extractall(CORPUS_DIR)\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    \"\"\"Save list of dicts to JSONL file.\"\"\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + '\\n')\n",
    "    print(f\"Saved {len(data)} items -> {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "build_index",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'mtrag_unified' found: 100000 vectors\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VECTOR INDEX VERIFICATION\n",
    "# ============================================================================\n",
    "# Check if the unified Qdrant collection exists; if not, build it.\n",
    "\n",
    "client = get_qdrant_client(QDRANT_PATH)\n",
    "collections = [c.name for c in client.get_collections().collections]\n",
    "\n",
    "if COLLECTION_NAME in collections:\n",
    "    info = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' found: {info.points_count} vectors\")\n",
    "else:\n",
    "    # Build index from scratch (only runs if collection missing)\n",
    "    print(\"Building vector index...\")\n",
    "    all_docs = []\n",
    "    for domain in DOMAINS:\n",
    "        path = get_corpus(domain)\n",
    "        if path:\n",
    "            docs = load_and_chunk_data(path)\n",
    "            # Tag with domain metadata for filtered retrieval\n",
    "            for d in docs:\n",
    "                d.metadata[\"domain\"] = domain\n",
    "            if not TEST_MODE:\n",
    "                docs = docs[:MAX_DOCS_PER_DOMAIN]\n",
    "            all_docs.extend(docs)\n",
    "    build_vector_store(all_docs, QDRANT_PATH, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "init_models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM: Llama 3.1 8B (4-bit NF4)...\n",
      "Creating Generation Components with model: meta-llama/Llama-3.1-8B-Instruct...\n",
      "Generation Components Ready.\n",
      "Initializing Self-CRAG Graph...\n",
      "All Systems Ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# --- Load quantized LLM (shared across Task B & C) ---\n",
    "print(\"Loading LLM: Llama 3.1 8B (4-bit NF4)...\")\n",
    "gen_components = create_generation_components()\n",
    "\n",
    "# --- Initialize Self-CRAG Graph for Task C ---\n",
    "print(\"Initializing Self-CRAG Graph...\")\n",
    "import src.graph\n",
    "src.graph._components = gen_components  # Share model to avoid OOM\n",
    "graph_app = initialize_graph()\n",
    "\n",
    "# --- Task B Chain (Direct LLM, no context) ---\n",
    "task_b_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert assistant. Answer based on your knowledge. Be concise.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "task_b_chain = task_b_prompt | gen_components.llm | StrOutputParser()\n",
    "\n",
    "print(\"All Systems Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 110 conversations from dataset.\n",
      "\n",
      "==================== DOMAIN: GOVT ====================\n",
      "Initializing retriever for domain: govt\n",
      "Processing 28 conversations...\n",
      "\n",
      "==================== DOMAIN: CLAPNQ ====================\n",
      "Initializing retriever for domain: clapnq\n",
      "Processing 29 conversations...\n",
      "\n",
      "==================== DOMAIN: FIQA ====================\n",
      "Initializing retriever for domain: fiqa\n",
      "Processing 27 conversations...\n",
      "\n",
      "==================== DOMAIN: CLOUD ====================\n",
      "Initializing retriever for domain: cloud\n",
      "Processing 26 conversations...\n",
      "\n",
      "Pipeline Complete. Total: 110 results per task.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN PIPELINE: TASK A, B, C\n",
    "# ============================================================================\n",
    "\n",
    "# --- Load Conversations ---\n",
    "with open(CONV_FILE) as f:\n",
    "    conversations = json.load(f)\n",
    "print(f\"Loaded {len(conversations)} conversations from dataset.\")\n",
    "\n",
    "# --- Result Containers ---\n",
    "results_A, results_B, results_C = [], [], []\n",
    "\n",
    "# --- Process Each Domain ---\n",
    "for domain in DOMAINS:\n",
    "    print(f\"\\n{'='*20} DOMAIN: {domain.upper()} {'='*20}\")\n",
    "\n",
    "    # Filter conversations by domain\n",
    "    convs_with_idx = [(i, c) for i, c in enumerate(conversations)\n",
    "                      if domain in c.get(\"domain\", \"\").lower()]\n",
    "    if TEST_MODE:\n",
    "        convs_with_idx = convs_with_idx[:TEST_QUERY_LIMIT]\n",
    "\n",
    "    # CRITICAL: Initialize domain-filtered retriever\n",
    "    print(f\"Initializing retriever for domain: {domain}\")\n",
    "    retriever = get_retriever(\n",
    "        qdrant_path=QDRANT_PATH,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        top_k_retrieve=20,  # BGE-M3 candidates\n",
    "        top_k_rerank=5,     # Cross-encoder final selection\n",
    "        domain=domain       # Domain filter for isolation\n",
    "    )\n",
    "\n",
    "    print(f\"Processing {len(convs_with_idx)} conversations...\")\n",
    "\n",
    "    for idx_in_domain, (global_idx, conv) in enumerate(tqdm(convs_with_idx, desc=domain, leave=False)):\n",
    "        msgs = conv.get(\"messages\", [])\n",
    "        q = extract_last_query(msgs)\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        conv_id = f\"{domain}_{idx_in_domain}\"\n",
    "\n",
    "        # ===================== TASK A: RETRIEVAL =====================\n",
    "        docs = retriever.invoke(q)\n",
    "        contexts = [{\n",
    "            \"document_id\": str(d.metadata.get(\"doc_id\", f\"{domain}_{i}\")),\n",
    "            \"score\": float(d.metadata.get(\"relevance_score\", 0.0)),\n",
    "            \"text\": d.metadata.get(\"parent_text\") or d.page_content\n",
    "        } for i, d in enumerate(docs)]\n",
    "\n",
    "        # ===================== TASK B: GENERATION (No Context) =====================\n",
    "        try:\n",
    "            ans_b = task_b_chain.invoke({\"question\": q})\n",
    "        except Exception as e:\n",
    "            ans_b = str(e)\n",
    "\n",
    "        # ===================== TASK C: RAG (Self-CRAG Graph) =====================\n",
    "        try:\n",
    "            chat_history = [\n",
    "                HumanMessage(content=m[\"text\"]) if m.get(\"speaker\") == \"user\"\n",
    "                else AIMessage(content=m[\"text\"])\n",
    "                for m in msgs\n",
    "            ]\n",
    "            response = graph_app.invoke({\n",
    "                \"question\": q,\n",
    "                \"domain\": domain,\n",
    "                \"messages\": chat_history\n",
    "            })\n",
    "            ans_c = response.get(\"generation\", \"I_DONT_KNOW\")\n",
    "            reason_c = response.get(\"fallback_reason\", \"none\")\n",
    "        except Exception as e:\n",
    "            print(f\"Task C Error: {e}\")\n",
    "            ans_c, reason_c = \"I_DONT_KNOW\", \"pipeline_error\"\n",
    "\n",
    "        # ===================== APPEND RESULTS =====================\n",
    "        results_A.append({\"conversation_id\": conv_id, \"original_index\": global_idx, \"ranking\": contexts})\n",
    "        results_B.append({\"conversation_id\": conv_id, \"original_index\": global_idx, \"answer\": ans_b})\n",
    "        results_C.append({\n",
    "            \"conversation_id\": conv_id,\n",
    "            \"original_index\": global_idx,\n",
    "            \"answer\": ans_c,\n",
    "            \"fallback_reason\": reason_c,\n",
    "            \"references\": contexts\n",
    "        })\n",
    "\n",
    "print(f\"\\nPipeline Complete. Total: {len(results_A)} results per task.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 110 items -> .../data/submissions/submission_TaskA_Gbgers.jsonl\n",
      "Saved 110 items -> .../data/submissions/submission_TaskB_Gbgers.jsonl\n",
      "Saved 110 items -> .../data/submissions/submission_TaskC_Gbgers.jsonl\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE SUBMISSION FILES\n",
    "# ============================================================================\n",
    "\n",
    "save_jsonl(results_A, FILE_A)\n",
    "save_jsonl(results_B, FILE_B)\n",
    "save_jsonl(results_C, FILE_C)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
