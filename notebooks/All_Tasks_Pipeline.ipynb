{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Complete Pipeline for All Tasks\n",
                "\n",
                "This notebook implements the complete RAG (Retrieval-Augmented Generation) pipeline for the MTRAGEval benchmark, generating submission files for all three evaluation tasks in a single execution.\n",
                "\n",
                "---\n",
                "\n",
                "### Task Overview\n",
                "\n",
                "| Task | Description | Output |\n",
                "|------|-------------|--------|\n",
                "| **Task A** | Document Retrieval | Top-K relevant passages for each query |\n",
                "| **Task B** | Standalone Generation | Answer using only LLM parametric knowledge |\n",
                "| **Task C** | RAG Generation | Answer using retrieved context + LLM |\n",
                "\n",
                "### Components\n",
                "\n",
                "| Component | Source | Description |\n",
                "|-----------|--------|-------------|\n",
                "| Retriever | `src.retrieval` | Dense Search (BGE) + Cross-Encoder Reranking |\n",
                "| Generator | `src.generation` | Llama-3.1-8B with constrained prompts |\n",
                "| Pipeline | `All_Tasks_Pipeline` | Unified execution loop |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kaggle_setup",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 0. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kaggle_env",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# KAGGLE/COLAB SETUP\n",
                "# ============================================================\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -q langchain langchain-community langchain-huggingface langchain-qdrant qdrant-client sentence-transformers bitsandbytes accelerate transformers tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Imports and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Project Root Detection\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "elif os.path.exists(\"llm-semeval-task8\"):\n",
                "    PROJECT_ROOT = \"llm-semeval-task8\"\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")\n",
                "\n",
                "# Import shared modules\n",
                "try:\n",
                "    from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "    from src.retrieval import get_retriever, get_qdrant_client\n",
                "    from src.generation import create_generation_components\n",
                "    # Import necessary LangChain components for custom chains\n",
                "    from langchain_core.prompts import PromptTemplate\n",
                "    from langchain_core.output_parsers import StrOutputParser\n",
                "except ImportError as e:\n",
                "    print(f\"Error importing modules: {e}\")\n",
                "    print(\"Ensure you are running from the project root or notebook directory.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# Execution Mode\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000\n",
                "TEST_QUERY_LIMIT = 5\n",
                "\n",
                "# Paths\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "\n",
                "# Output Files\n",
                "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages: list) -> str:\n",
                "    \"\"\"Extract the most recent user query from a conversation.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "def get_corpus_file(domain: str) -> str:\n",
                "    \"\"\"Get or extract corpus file path.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "def save_jsonl(data: list, path: str) -> None:\n",
                "    \"\"\"Save list of dicts to JSONL file.\"\"\"\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for item in data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    print(f\"Saved {len(data)} items to {path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "index_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Build Unified Vector Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_index",
            "metadata": {},
            "outputs": [],
            "source": [
                "need_build = True\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"Collection found: {info.points_count} vectors\")\n",
                "            need_build = False\n",
                "    except: pass\n",
                "\n",
                "if need_build:\n",
                "    print(f\"Building collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    for domain in DOMAINS:\n",
                "        path = get_corpus_file(domain)\n",
                "        if not path: continue\n",
                "        docs = load_and_chunk_data(path)\n",
                "        for doc in docs: doc.metadata[\"domain\"] = domain\n",
                "        if TEST_MODE: docs = docs[:TEST_SUBSET_SIZE]\n",
                "        all_docs.extend(docs)\n",
                "    \n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"Index built.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "init_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Initialize Components (Retriever & Generator)\n",
                "\n",
                "We use the unified `create_generation_components` factory to load the LLM (Llama 3.1) and all associated chains."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_components",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize Retriever\n",
                "print(\"Initializing Retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME\n",
                ")\n",
                "\n",
                "# 2. Initialize Generation Components (LLM + Chains)\n",
                "# This loads Llama 3.1 8B Instruct with 4-bit quantization\n",
                "print(\"Initializing Generator (Llama 3.1 8B)...\")\n",
                "gen_components = create_generation_components(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
                "\n",
                "# 3. Create Custom Chain for Task B (Standalone Generation)\n",
                "# Since Task B is NOT RAG, we need a separate prompt that doesn't use retrieved documents\n",
                "task_b_prompt = PromptTemplate(\n",
                "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are an expert assistant with comprehensive knowledge. \n",
                "Task: Answer the user's question based solely on your internal knowledge. Do not use external documents.\n",
                "Response: Be concise, accurate, and professional.<|eot_id|>\n",
                "<|start_header_id|>user<|end_header_id|>\n",
                "{question}<|eot_id|>\n",
                "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
                "    input_variables=[\"question\"]\n",
                ")\n",
                "# Create chain reusing the LLM from gen_components\n",
                "task_b_chain = task_b_prompt | gen_components.llm | StrOutputParser()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execute_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Execute Unified Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "\n",
                "results_A, results_B, results_C = [], [], []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*40}\\nProcessing Domain: {domain.upper()}\\n{'='*40}\")\n",
                "    \n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    if TEST_MODE:\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=domain):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        if not query: continue\n",
                "        \n",
                "        # --- TASK A: Retrieval ---\n",
                "        docs = retriever.invoke(query)\n",
                "        \n",
                "        # Format Contexts\n",
                "        contexts = []\n",
                "        context_text = \"\"\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            text = meta.get(\"parent_text\") or doc.page_content\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": text\n",
                "            })\n",
                "            context_text += f\"[Document {i+1}]\\n{text}\\n\\n\"\n",
                "        \n",
                "        # --- TASK B: Standalone Generation ---\n",
                "        try:\n",
                "            answer_b = task_b_chain.invoke({\"question\": query})\n",
                "        except Exception as e:\n",
                "            answer_b = str(e)\n",
                "        \n",
                "        # --- TASK C: RAG Generation (using src.generation.generator) ---\n",
                "        try:\n",
                "            # Uses key 'context' and 'question' as required by src/generation.py template\n",
                "            answer_c = gen_components.generator.invoke({\"context\": context_text, \"question\": query})\n",
                "        except Exception as e:\n",
                "            answer_c = str(e)\n",
                "\n",
                "        # --- Collect Results ---\n",
                "        base = {\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages]\n",
                "        }\n",
                "        \n",
                "        results_A.append({**base, \"contexts\": contexts})\n",
                "        results_B.append({**base, \"predictions\": [{\"text\": answer_b}]})\n",
                "        results_C.append({**base, \"contexts\": contexts, \"predictions\": [{\"text\": answer_c}]})\n",
                "\n",
                "print(f\"\\nProcessing Complete. Total Conversations: {len(results_A)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Saving submission files...\")\n",
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)\n",
                "print(\"Done.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}