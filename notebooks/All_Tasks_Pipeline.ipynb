{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Complete Pipeline for All Tasks\n",
                "\n",
                "This notebook implements the complete RAG (Retrieval-Augmented Generation) pipeline for the MTRAGEval benchmark, generating submission files for all three evaluation tasks in a single execution.\n",
                "\n",
                "---\n",
                "\n",
                "### Task Overview\n",
                "\n",
                "| Task | Description | Output |\n",
                "|------|-------------|--------|\n",
                "| **Task A** | Document Retrieval | Top-K relevant passages for each query |\n",
                "| **Task B** | Standalone Generation | Answer using only LLM parametric knowledge |\n",
                "| **Task C** | RAG Generation | Answer using retrieved context + LLM |\n",
                "\n",
                "### Pipeline Architecture\n",
                "\n",
                "```\n",
                "Conversation --> Query Extraction --> Retrieval (Task A)\n",
                "                                          |\n",
                "                                          v\n",
                "                          +---------------+---------------+\n",
                "                          |                               |\n",
                "                    Task B: LLM Only              Task C: RAG\n",
                "                    (No Context)                  (With Context)\n",
                "                          |                               |\n",
                "                          v                               v\n",
                "                   predictions.jsonl              predictions.jsonl\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kaggle_setup",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 0. Environment Setup (Kaggle/Colab)\n",
                "\n",
                "Execute this cell first when running on Kaggle or Google Colab to clone the repository and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kaggle_env",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# KAGGLE/COLAB ENVIRONMENT SETUP\n",
                "# Uncomment the lines below when running on cloud platforms\n",
                "# ============================================================\n",
                "\n",
                "# import os\n",
                "# \n",
                "# # Clone repository if not present\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "# \n",
                "# # Change to project directory\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# \n",
                "# # Install dependencies\n",
                "# !pip install -q \\\n",
                "#     langchain \\\n",
                "#     langchain-community \\\n",
                "#     langchain-huggingface \\\n",
                "#     langchain-qdrant \\\n",
                "#     qdrant-client \\\n",
                "#     sentence-transformers \\\n",
                "#     bitsandbytes \\\n",
                "#     accelerate \\\n",
                "#     transformers \\\n",
                "#     tqdm\n",
                "# \n",
                "# # Verify GPU availability\n",
                "# import torch\n",
                "# print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "# if torch.cuda.is_available():\n",
                "#     print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
                "#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Imports and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# ============================================================\n",
                "# PROJECT ROOT DETECTION\n",
                "# Automatically detect project root for both local and cloud\n",
                "# ============================================================\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "elif os.path.exists(\"llm-semeval-task8\"):\n",
                "    PROJECT_ROOT = \"llm-semeval-task8\"\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "# Team Information\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "\n",
                "# Domains to process\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Retriever Configuration\n",
                "TOP_K_RETRIEVE = 20   # Candidates before reranking (high recall)\n",
                "TOP_K_RERANK = 5      # Final documents after reranking (high precision)\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# Execution Mode\n",
                "# Set TEST_MODE = False for full submission\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000   # Chunks per domain (indexing)\n",
                "TEST_QUERY_LIMIT = 5      # Conversations per domain (inference)\n",
                "\n",
                "# Path Configuration\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "\n",
                "# Output Files\n",
                "FILE_A = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "FILE_B = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "FILE_C = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "# Status\n",
                "mode_str = \"TEST MODE\" if TEST_MODE else \"FULL MODE\"\n",
                "print(f\"Execution Mode: {mode_str}\")\n",
                "if TEST_MODE:\n",
                "    print(f\"  - Index subset: {TEST_SUBSET_SIZE} chunks/domain\")\n",
                "    print(f\"  - Query subset: {TEST_QUERY_LIMIT} conversations/domain\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages: list) -> str:\n",
                "    \"\"\"\n",
                "    Extract the most recent user query from a conversation.\n",
                "    \n",
                "    Args:\n",
                "        messages: List of message dictionaries with 'speaker' and 'text' keys.\n",
                "        \n",
                "    Returns:\n",
                "        The text of the last user message, or empty string if not found.\n",
                "    \"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "\n",
                "def get_corpus_file(domain: str) -> str:\n",
                "    \"\"\"\n",
                "    Get the path to a domain corpus file, extracting from ZIP if necessary.\n",
                "    \n",
                "    Args:\n",
                "        domain: Domain name (e.g., 'govt', 'fiqa').\n",
                "        \n",
                "    Returns:\n",
                "        Path to the JSONL file, or None if not available.\n",
                "    \"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"Extracting {domain}.jsonl from archive...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "\n",
                "def save_jsonl(data: list, path: str) -> None:\n",
                "    \"\"\"\n",
                "    Save a list of dictionaries to a JSONL file.\n",
                "    \n",
                "    Args:\n",
                "        data: List of dictionaries to save.\n",
                "        path: Output file path.\n",
                "    \"\"\"\n",
                "    with open(path, 'w', encoding='utf-8') as f:\n",
                "        for item in data:\n",
                "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "    print(f\"Saved {len(data)} items to {path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "indexing_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Build Unified Vector Index\n",
                "\n",
                "This step creates a single Qdrant collection containing embeddings from all four domains. The unified index enables cross-domain retrieval and reduces memory overhead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_index",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "\n",
                "# Check for existing collection\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"Found existing collection '{COLLECTION_NAME}' with {info.points_count} vectors.\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"Building unified collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            print(f\"Warning: Corpus not found for domain '{domain}'\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        # Add domain metadata for filtering\n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"  Limiting to {TEST_SUBSET_SIZE} chunks (test mode)\")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"  Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"Total documents to index: {len(all_docs)}\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"Index construction complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Initialize Retriever and Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# RETRIEVER INITIALIZATION\n",
                "# Two-stage retrieval: Dense search -> Cross-encoder reranking\n",
                "# ============================================================\n",
                "print(\"Initializing retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"Retriever initialized successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LANGUAGE MODEL INITIALIZATION\n",
                "# Using 4-bit quantization for memory efficiency\n",
                "# ============================================================\n",
                "import torch\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
                "\n",
                "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
                "\n",
                "print(f\"Loading language model: {MODEL_ID}\")\n",
                "\n",
                "# 4-bit quantization configuration\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "        repetition_penalty=1.1,\n",
                "        return_full_text=False,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "    llm = HuggingFacePipeline(pipeline=pipe)\n",
                "    print(\"Language model loaded successfully.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    print(\"Falling back to dummy LLM for pipeline testing.\")\n",
                "    from langchain.llms.fake import FakeListLLM\n",
                "    llm = FakeListLLM(responses=[\"[Dummy response - model not loaded]\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompts_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Prompt Templates\n",
                "\n",
                "The following prompts are designed using established prompt engineering principles:\n",
                "\n",
                "- **Role Assignment**: Defines the assistant's expertise and behavior\n",
                "- **Task Specification**: Clear instructions on what to produce\n",
                "- **Format Constraints**: Output structure requirements\n",
                "- **Grounding**: Context-based answering for Task C"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompts",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TASK B: STANDALONE GENERATION (No Context)\n",
                "# The model must rely solely on its parametric knowledge.\n",
                "# ============================================================\n",
                "PROMPT_TASK_B = \"\"\"You are an expert assistant with comprehensive knowledge across government policy, technology, and finance domains.\n",
                "\n",
                "Your task is to provide a direct, accurate, and informative answer to the user's question based solely on your training knowledge.\n",
                "\n",
                "Guidelines:\n",
                "- Provide factual, well-structured responses\n",
                "- If uncertain, acknowledge limitations rather than fabricating information\n",
                "- Be concise but thorough\n",
                "- Use clear, professional language\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "# ============================================================\n",
                "# TASK C: RAG GENERATION (With Retrieved Context)\n",
                "# The model must ground its response in the provided documents.\n",
                "# ============================================================\n",
                "PROMPT_TASK_C = \"\"\"You are an expert assistant specializing in document-grounded question answering.\n",
                "\n",
                "Your task is to answer the user's question using ONLY the information provided in the context below. Follow these guidelines strictly:\n",
                "\n",
                "Guidelines:\n",
                "1. Base your answer exclusively on the provided context\n",
                "2. If the context contains relevant information, synthesize it into a coherent response\n",
                "3. If the context does not contain sufficient information to answer the question, explicitly state: \"The provided documents do not contain enough information to answer this question.\"\n",
                "4. Do not introduce external knowledge not present in the context\n",
                "5. Maintain accuracy and avoid speculation\n",
                "\n",
                "---\n",
                "CONTEXT:\n",
                "{context}\n",
                "---\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\"\n",
                "\n",
                "\n",
                "def generate_answer(question: str, context: str = None) -> str:\n",
                "    \"\"\"\n",
                "    Generate an answer using the appropriate prompt template.\n",
                "    \n",
                "    Args:\n",
                "        question: The user's question.\n",
                "        context: Optional retrieved context for RAG generation.\n",
                "        \n",
                "    Returns:\n",
                "        Generated answer string.\n",
                "    \"\"\"\n",
                "    if llm is None:\n",
                "        return \"[Error: Language model not initialized]\"\n",
                "    \n",
                "    if context:\n",
                "        prompt = PROMPT_TASK_C.format(question=question, context=context)\n",
                "    else:\n",
                "        prompt = PROMPT_TASK_B.format(question=question)\n",
                "    \n",
                "    try:\n",
                "        return llm.invoke(prompt)\n",
                "    except Exception as e:\n",
                "        return f\"[Generation error: {e}]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Pipeline Execution\n",
                "\n",
                "Process all conversations and generate outputs for all three tasks simultaneously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load conversation data\n",
                "print(\"Loading conversation data...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Loaded {len(all_conversations)} conversations.\")\n",
                "\n",
                "# Result containers\n",
                "results_A = []  # Task A: Retrieval\n",
                "results_B = []  # Task B: Generation (no context)\n",
                "results_C = []  # Task C: RAG (with context)\n",
                "\n",
                "# Process each domain\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Processing Domain: {domain.upper()}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Filter conversations by domain\n",
                "    domain_convs = [\n",
                "        c for c in all_conversations \n",
                "        if domain.lower() in c.get(\"domain\", \"\").lower()\n",
                "    ]\n",
                "    print(f\"Found {len(domain_convs)} conversations.\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    # Apply test mode limit\n",
                "    if TEST_MODE:\n",
                "        print(f\"Test mode: limiting to {TEST_QUERY_LIMIT} conversations.\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    # Process conversations\n",
                "    for conv in tqdm(domain_convs, desc=f\"{domain}\"):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # --- TASK A: Retrieval ---\n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Retrieval error: {e}\")\n",
                "            docs = []\n",
                "        \n",
                "        # Format retrieved contexts\n",
                "        contexts = []\n",
                "        context_text = \"\"\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            parent_text = meta.get(\"parent_text\") or doc.page_content\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": parent_text\n",
                "            })\n",
                "            context_text += parent_text + \"\\n\\n\"\n",
                "        \n",
                "        # --- TASK B: Generation (standalone) ---\n",
                "        answer_b = generate_answer(query, context=None)\n",
                "        \n",
                "        # --- TASK C: RAG Generation ---\n",
                "        answer_c = generate_answer(query, context=context_text.strip())\n",
                "        \n",
                "        # --- Format Results ---\n",
                "        base_result = {\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages]\n",
                "        }\n",
                "        \n",
                "        # Task A result\n",
                "        result_a = base_result.copy()\n",
                "        result_a[\"contexts\"] = contexts\n",
                "        results_A.append(result_a)\n",
                "        \n",
                "        # Task B result\n",
                "        result_b = base_result.copy()\n",
                "        result_b[\"predictions\"] = [{\"text\": answer_b}]\n",
                "        results_B.append(result_b)\n",
                "        \n",
                "        # Task C result\n",
                "        result_c = base_result.copy()\n",
                "        result_c[\"contexts\"] = contexts\n",
                "        result_c[\"predictions\"] = [{\"text\": answer_c}]\n",
                "        results_C.append(result_c)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"Processing Complete\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"Task A results: {len(results_A)}\")\n",
                "print(f\"Task B results: {len(results_B)}\")\n",
                "print(f\"Task C results: {len(results_C)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Save Submission Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Saving submission files...\")\n",
                "print()\n",
                "\n",
                "save_jsonl(results_A, FILE_A)\n",
                "save_jsonl(results_B, FILE_B)\n",
                "save_jsonl(results_C, FILE_C)\n",
                "\n",
                "print()\n",
                "print(\"All submission files saved successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "validation_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Output Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "validate",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Validating submission files...\")\n",
                "print()\n",
                "\n",
                "def validate_task(results: list, task: str) -> tuple:\n",
                "    \"\"\"\n",
                "    Validate the structure of task results.\n",
                "    \n",
                "    Returns:\n",
                "        (is_valid, message) tuple\n",
                "    \"\"\"\n",
                "    if not results:\n",
                "        return False, \"No results generated\"\n",
                "    \n",
                "    sample = results[0]\n",
                "    \n",
                "    if task == \"A\":\n",
                "        valid = \"contexts\" in sample and isinstance(sample[\"contexts\"], list)\n",
                "    elif task == \"B\":\n",
                "        valid = \"predictions\" in sample and isinstance(sample[\"predictions\"], list)\n",
                "    elif task == \"C\":\n",
                "        valid = \"contexts\" in sample and \"predictions\" in sample\n",
                "    else:\n",
                "        valid = False\n",
                "    \n",
                "    return valid, \"Valid\" if valid else \"Invalid structure\"\n",
                "\n",
                "\n",
                "validation_results = [\n",
                "    (\"Task A (Retrieval)\", results_A, \"A\"),\n",
                "    (\"Task B (Generation)\", results_B, \"B\"),\n",
                "    (\"Task C (RAG)\", results_C, \"C\")\n",
                "]\n",
                "\n",
                "all_valid = True\n",
                "for name, results, task in validation_results:\n",
                "    valid, msg = validate_task(results, task)\n",
                "    status = \"PASS\" if valid else \"FAIL\"\n",
                "    print(f\"  {name}: {status} ({msg})\")\n",
                "    all_valid = all_valid and valid\n",
                "\n",
                "print()\n",
                "if all_valid:\n",
                "    print(\"All validations passed. Ready for submission.\")\n",
                "else:\n",
                "    print(\"Validation errors detected. Please review the output.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}