{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MTRAGEval - RAG Pipeline Testing\n",
                "\n",
                "This notebook tests the complete Self-CRAG pipeline for SemEval 2026 Task 8.\n",
                "\n",
                "## Steps:\n",
                "1. Install dependencies and load Llama 3.1\n",
                "2. Initialize retrieval components\n",
                "3. Build and test the LangGraph workflow\n",
                "4. Run multi-turn conversations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (run once)\n",
                "!pip install -q langchain==0.1.10 langchain-community==0.0.25 langchain-huggingface==0.0.3 langgraph==0.0.26\n",
                "!pip install -q transformers accelerate bitsandbytes\n",
                "!pip install -q chromadb==0.4.24 sentence-transformers==2.5.1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, '../')\n",
                "\n",
                "# Verify GPU\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    for i in range(torch.cuda.device_count()):\n",
                "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
                "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# HuggingFace login for gated models (Llama 3.1)\n",
                "from huggingface_hub import login\n",
                "\n",
                "# Use Kaggle secrets or environment variable\n",
                "# login(token=\"your_hf_token_here\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Llama 3.1 (4-bit Quantized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "\n",
                "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
                "\n",
                "def load_llama_4bit():\n",
                "    \"\"\"\n",
                "    Load Llama 3.1 with 4-bit NF4 quantization.\n",
                "    \n",
                "    TODO: Implement model loading with BitsAndBytesConfig\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement 4-bit Llama loading\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load LLM\n",
                "# print(\"Loading Llama 3.1 8B (4-bit)... This may take 1-2 minutes.\")\n",
                "# llm = load_llama_4bit()\n",
                "# print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Retrieval Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.vectorstores import Chroma\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "from langchain.retrievers import ContextualCompressionRetriever\n",
                "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
                "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
                "\n",
                "CHROMA_PERSIST_DIR = \"../chromadb\"\n",
                "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
                "RERANKER_MODEL = \"BAAI/bge-reranker-v2-m3\"\n",
                "\n",
                "def get_retriever_with_reranking():\n",
                "    \"\"\"\n",
                "    Build retriever: Vector Search (Top 20) -> Rerank (Top 5).\n",
                "    \n",
                "    TODO: Implement retriever with cross-encoder reranking\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement retriever with reranking\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize retriever\n",
                "# retriever = get_retriever_with_reranking()\n",
                "# print(\"Retriever initialized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create LangChain Chains"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
                "from langchain_core.pydantic_v1 import BaseModel, Field\n",
                "\n",
                "def create_query_rewriter(llm):\n",
                "    \"\"\"\n",
                "    Create query rewriter chain for context-dependent questions.\n",
                "    \n",
                "    TODO: Implement with Llama 3 special tokens\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement query rewriter\")\n",
                "\n",
                "\n",
                "def create_generator(llm):\n",
                "    \"\"\"\n",
                "    Create RAG generator with I_DONT_KNOW fallback.\n",
                "    \n",
                "    TODO: Implement generator chain\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement generator\")\n",
                "\n",
                "\n",
                "def create_relevance_grader(llm):\n",
                "    \"\"\"\n",
                "    Create document relevance grader (CRAG).\n",
                "    \n",
                "    TODO: Implement with JSON output\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement relevance grader\")\n",
                "\n",
                "\n",
                "def create_hallucination_grader(llm):\n",
                "    \"\"\"\n",
                "    Create hallucination grader (Self-RAG).\n",
                "    \n",
                "    TODO: Implement with JSON output\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement hallucination grader\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build Self-CRAG Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import TypedDict, List, Annotated, Any\n",
                "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
                "from langgraph.graph import StateGraph, END, START\n",
                "from langgraph.graph.message import add_messages\n",
                "\n",
                "class GraphState(TypedDict):\n",
                "    \"\"\"State for Self-CRAG workflow.\"\"\"\n",
                "    messages: Annotated[List[BaseMessage], add_messages]\n",
                "    question: str\n",
                "    standalone_question: str\n",
                "    documents: List[Any]\n",
                "    generation: str\n",
                "    documents_relevant: str  # 'yes' or 'no'\n",
                "    is_hallucination: str    # 'yes' or 'no'\n",
                "    retry_count: int"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_self_crag_graph(retriever, query_rewriter, generator, relevance_grader, hallucination_grader):\n",
                "    \"\"\"\n",
                "    Build the Self-CRAG LangGraph workflow.\n",
                "    \n",
                "    Nodes:\n",
                "    - rewrite: Rewrite context-dependent queries\n",
                "    - retrieve: Search vector store\n",
                "    - grade_docs: Filter irrelevant documents (CRAG)\n",
                "    - generate: Produce answer\n",
                "    - hallucination_check: Validate generation (Self-RAG)\n",
                "    - fallback: Return I_DONT_KNOW\n",
                "    \n",
                "    TODO: Implement graph construction\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement Self-CRAG graph\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build graph\n",
                "# app = build_self_crag_graph(retriever, query_rewriter, generator, relevance_grader, hallucination_grader)\n",
                "# print(\"Self-CRAG graph built!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Single Turn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_single_turn(app, question: str):\n",
                "    \"\"\"\n",
                "    Run a single question through the pipeline.\n",
                "    \n",
                "    TODO: Implement single turn execution\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement single turn\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test single turn\n",
                "# question = \"Who is the CEO of Apple?\"\n",
                "# response = run_single_turn(app, question)\n",
                "# print(f\"Q: {question}\")\n",
                "# print(f\"A: {response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test Multi-Turn Conversation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_multi_turn_conversation(app, questions: list):\n",
                "    \"\"\"\n",
                "    Run a multi-turn conversation maintaining history.\n",
                "    \n",
                "    TODO: Implement multi-turn with chat history\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement multi-turn conversation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test multi-turn\n",
                "# questions = [\n",
                "#     \"Who is the CEO of Apple?\",\n",
                "#     \"How old is he?\",\n",
                "#     \"When did he become CEO?\"\n",
                "# ]\n",
                "# run_multi_turn_conversation(app, questions)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. VRAM Monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monitor GPU memory usage\n",
                "!nvidia-smi"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}