{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Task A: Retrieval\n",
                "\n",
                "This notebook implements **Task A: Retrieval** for MTRAGEval.\n",
                "\n",
                "Uses a **UNIFIED collection** for all domains (govt, clapnq, fiqa, cloud).\n",
                "\n",
                "**Goal:** Given a conversation, retrieve the top-K most relevant documents."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è TEST MODE: Indexing 1000 chunks/domain, 10 queries/domain\n"
                    ]
                }
            ],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "TOP_K_RETRIEVE = 20\n",
                "TOP_K_RERANK = 5\n",
                "\n",
                "# UNIFIED COLLECTION NAME\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# TEST MODE: Set to True for quick verification\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000   # Number of chunks to index per domain\n",
                "TEST_QUERY_LIMIT = 10     # Number of queries to process per domain\n",
                "\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "if TEST_MODE:\n",
                "    print(f\"‚ö†Ô∏è TEST MODE: Indexing {TEST_SUBSET_SIZE} chunks/domain, {TEST_QUERY_LIMIT} queries/domain\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers",
            "metadata": {},
            "source": [
                "## 2. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "helper_funcs",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages):\n",
                "    \"\"\"Extract last user question from messages.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "def get_corpus_file(domain):\n",
                "    \"\"\"Get or extract corpus file path.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"üì¶ Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "build_section",
            "metadata": {},
            "source": [
                "## 3. Build Unified Collection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "build_unified",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîÑ Building unified collection 'mtrag_unified' with all domains...\n",
                        "üìÇ Loading govt...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/govt.jsonl ---\n",
                        "Loaded 49607 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "‚úÇÔ∏è TEST MODE: Slicing govt to 1000 chunks\n",
                        "   Added 1000 chunks\n",
                        "üìÇ Loading clapnq...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/clapnq.jsonl ---\n",
                        "Loaded 183408 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "‚úÇÔ∏è TEST MODE: Slicing clapnq to 1000 chunks\n",
                        "   Added 1000 chunks\n",
                        "üìÇ Loading fiqa...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/fiqa.jsonl ---\n",
                        "Loaded 60984 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "‚úÇÔ∏è TEST MODE: Slicing fiqa to 1000 chunks\n",
                        "   Added 1000 chunks\n",
                        "üìÇ Loading cloud...\n",
                        "--- LOADING DATA FROM /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/dataset/corpora/passage_level/cloud.jsonl ---\n",
                        "Loaded 72439 documents.\n",
                        "--- STARTING PARENT-CHILD SPLITTING ---\n",
                        "‚úÇÔ∏è TEST MODE: Slicing cloud to 1000 chunks\n",
                        "   Added 1000 chunks\n",
                        "üìä Total documents: 4000\n",
                        "--- BUILDING VECTOR STORE: mtrag_unified ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/src/ingestion.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  _embedding_model = HuggingFaceEmbeddings(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   Adding 4000 documents in batches of 64...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Indexing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [01:59<00:00,  1.90s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- VECTOR STORE BUILT AND SAVED ---\n",
                        "‚úÖ Unified collection built\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Check if collection already exists\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"‚úÖ Unified collection '{COLLECTION_NAME}' exists ({info.points_count} vectors)\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"üîÑ Building unified collection '{COLLECTION_NAME}' with all domains...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            print(f\"‚ö†Ô∏è Corpus not found for {domain}, skipping...\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"üìÇ Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        # Add domain metadata\n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"‚úÇÔ∏è TEST MODE: Slicing {domain} to {TEST_SUBSET_SIZE} chunks\")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"   Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"üìä Total documents: {len(all_docs)}\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"‚úÖ Unified collection built\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "## 4. Initialize Unified Retriever"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Initializing unified retriever...\n",
                        "üîß Loading reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
                        "‚úÖ Retriever ready\n"
                    ]
                }
            ],
            "source": [
                "print(\"üîç Initializing unified retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"‚úÖ Retriever ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "run",
            "metadata": {},
            "source": [
                "## 5. Run Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "main_loop",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÇ Loading conversations...\n",
                        "Total conversations: 110\n",
                        "\n",
                        "========================================\n",
                        "üåç DOMAIN: GOVT\n",
                        "========================================\n",
                        "Found 28 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 10 queries\n",
                        "üöÄ Running retrieval...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 14.48it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "üåç DOMAIN: CLAPNQ\n",
                        "========================================\n",
                        "Found 29 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 10 queries\n",
                        "üöÄ Running retrieval...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 15.66it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "üåç DOMAIN: FIQA\n",
                        "========================================\n",
                        "Found 27 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 10 queries\n",
                        "üöÄ Running retrieval...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 21.70it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "üåç DOMAIN: CLOUD\n",
                        "========================================\n",
                        "Found 26 conversations\n",
                        "‚úÇÔ∏è TEST MODE: Processing 10 queries\n",
                        "üöÄ Running retrieval...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 19.50it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "‚úÖ Total results: 40\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "all_results = []\n",
                "\n",
                "# Load ALL conversations\n",
                "print(\"üìÇ Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Total conversations: {len(all_conversations)}\")\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*40}\\nüåç DOMAIN: {domain.upper()}\\n{'='*40}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "        \n",
                "    if TEST_MODE:\n",
                "        print(f\"‚úÇÔ∏è TEST MODE: Processing {TEST_QUERY_LIMIT} queries\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    print(f\"üöÄ Running retrieval...\")\n",
                "    for conv in tqdm(domain_convs):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        if not query: \n",
                "            continue\n",
                "            \n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {e}\")\n",
                "            docs = []\n",
                "            \n",
                "        # Format output\n",
                "        contexts = []\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": meta.get(\"parent_text\") or doc.page_content\n",
                "            })\n",
                "            \n",
                "        all_results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages],\n",
                "            \"contexts\": contexts\n",
                "        })\n",
                "\n",
                "print(f\"\\n‚úÖ Total results: {len(all_results)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save",
            "metadata": {},
            "source": [
                "## 6. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "save_file",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Saving 40 results to /home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/data/submissions/submission_TaskA_Gbgers.jsonl...\n",
                        "‚úÖ Done!\n",
                        "\u001b[92mVALIDATION PASS: Structure correct.\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "print(f\"üíæ Saving {len(all_results)} results to {OUTPUT_FILE}...\")\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "print(\"‚úÖ Done!\")\n",
                "\n",
                "# Validation\n",
                "if all_results:\n",
                "    sample = all_results[0]\n",
                "    if \"contexts\" in sample and isinstance(sample[\"contexts\"], list):\n",
                "        print(\"\\033[92mVALIDATION PASS: Structure correct.\\033[0m\")\n",
                "    else:\n",
                "        print(\"\\033[91mVALIDATION FAIL: Key 'contexts' missing or invalid.\\033[0m\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
