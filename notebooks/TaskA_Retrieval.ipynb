{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Task A: Document Retrieval\n",
                "\n",
                "This notebook implements **Task A** of the MTRAGEval benchmark, which evaluates the quality of document retrieval in a multi-turn conversational setting.\n",
                "\n",
                "---\n",
                "\n",
                "### Objective\n",
                "\n",
                "Given a multi-turn conversation, retrieve the top-K most relevant documents from a domain-specific corpus that can be used to answer the user's question.\n",
                "\n",
                "### Retrieval Pipeline\n",
                "\n",
                "```\n",
                "Query --> Dense Retrieval (BGE-M3) --> Top-20 Candidates\n",
                "                                            |\n",
                "                                            v\n",
                "                                   Cross-Encoder Reranking\n",
                "                                            |\n",
                "                                            v\n",
                "                                      Top-5 Documents\n",
                "```\n",
                "\n",
                "### Components\n",
                "\n",
                "| Component | Model | Purpose |\n",
                "|-----------|-------|--------|\n",
                "| Embeddings | `BAAI/bge-m3` | Dense vector representation |\n",
                "| Reranker | `BAAI/bge-reranker-v2-m3` | Cross-encoder scoring |\n",
                "| Vector Store | Qdrant | Efficient similarity search |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "# Project root detection\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Retriever settings\n",
                "TOP_K_RETRIEVE = 20   # Initial dense retrieval\n",
                "TOP_K_RERANK = 5      # After cross-encoder reranking\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# Execution mode\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000\n",
                "TEST_QUERY_LIMIT = 10\n",
                "\n",
                "# Paths\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
                "print(f\"Output: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helper_funcs",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages: list) -> str:\n",
                "    \"\"\"Extract the most recent user query from a conversation.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "\n",
                "def get_corpus_file(domain: str) -> str:\n",
                "    \"\"\"Get corpus file path, extracting from ZIP if necessary.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "build_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Build Unified Vector Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_unified",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for existing collection\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"Existing collection found: {info.points_count} vectors\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"Building collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            print(f\"Warning: Corpus not found for {domain}\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"  Limiting to {TEST_SUBSET_SIZE} chunks\")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"  Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"Total: {len(all_docs)} documents\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"Index built successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Initialize Retriever"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"Retriever ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "run_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Execute Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "main_loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load conversations\n",
                "print(\"Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Loaded {len(all_conversations)} conversations.\")\n",
                "\n",
                "all_results = []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Domain: {domain.upper()}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations.\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    if TEST_MODE:\n",
                "        print(f\"Test mode: limiting to {TEST_QUERY_LIMIT} queries.\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=domain):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # Retrieve documents\n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {e}\")\n",
                "            docs = []\n",
                "        \n",
                "        # Format contexts\n",
                "        contexts = []\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": meta.get(\"parent_text\") or doc.page_content\n",
                "            })\n",
                "        \n",
                "        # Build result\n",
                "        all_results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages],\n",
                "            \"contexts\": contexts\n",
                "        })\n",
                "\n",
                "print(f\"\\nTotal results: {len(all_results)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_file",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Saving {len(all_results)} results to {OUTPUT_FILE}...\")\n",
                "\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "\n",
                "print(\"Saved successfully.\")\n",
                "\n",
                "# Validation\n",
                "if all_results:\n",
                "    sample = all_results[0]\n",
                "    if \"contexts\" in sample and isinstance(sample[\"contexts\"], list):\n",
                "        print(\"Validation: PASS - Structure correct.\")\n",
                "    else:\n",
                "        print(\"Validation: FAIL - Invalid structure.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}