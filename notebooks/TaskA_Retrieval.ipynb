{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "task-a-header",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Task A: Retrieval (All Domains)\n",
                "\n",
                "This notebook implements **Task A: Passage Retrieval** for MTRAGEval.\n",
                "\n",
                "**Goal:** Retrieve relevant documents for **ALL 4 DOMAINS** (`govt`, `clapnq`, `fiqa`, `cloud`).\n",
                "\n",
                "**Process:**\n",
                "1. Iterate through each domain.\n",
                "2. Build/Load dedicated Qdrant collection.\n",
                "3. Filter conversations for that domain.\n",
                "4. Retrieve and Rerank.\n",
                "5. Save combined results.\n",
                "\n",
                "**Output:** Single JSONL file with all results.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-section",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever\n",
                "from qdrant_client import QdrantClient"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing domains: ['govt', 'clapnq', 'fiqa', 'cloud']\n",
                        "‚ö†Ô∏è TEST MODE ACTIVE: Indexing only 1000 chunks, processing 10 queries.\n"
                    ]
                }
            ],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]  # All domains\n",
                "TOP_K_RETRIEVE = 20\n",
                "TOP_K_RERANK = 5\n",
                "\n",
                "# TEST MODE: Set to True for quick verification\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000   # Number of chunks to index per domain (if building new)\n",
                "TEST_QUERY_LIMIT = 10     # Number of queries to process per domain\n",
                "\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskA_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Processing domains: {DOMAINS}\")\n",
                "if TEST_MODE:\n",
                "    print(f\"‚ö†Ô∏è TEST MODE ACTIVE: Indexing only {TEST_SUBSET_SIZE} chunks, processing {TEST_QUERY_LIMIT} queries.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helper-functions",
            "metadata": {},
            "source": [
                "## 2. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(turns):\n",
                "    \"\"\"Extract last user question from turns.\"\"\"\n",
                "    for turn in reversed(turns):\n",
                "        if turn.get(\"role\") == \"user\":\n",
                "            return turn.get(\"content\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "def get_corpus_file(domain):\n",
                "    \"\"\"Get or extract corpus file path.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"üì¶ Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "def ensure_vector_store(domain):\n",
                "    \"\"\"Ensure Qdrant collection exists for domain.\"\"\"\n",
                "    collection_name = f\"mtrag_{domain}\"\n",
                "    corpus_path = get_corpus_file(domain)\n",
                "    \n",
                "    if not corpus_path:\n",
                "        print(f\"‚ö†Ô∏è Corpus not found for {domain}\")\n",
                "        return None\n",
                "    \n",
                "    need_build = True\n",
                "    if os.path.exists(QDRANT_PATH):\n",
                "        client = None\n",
                "        try:\n",
                "            client = QdrantClient(path=QDRANT_PATH)\n",
                "            if client.collection_exists(collection_name):\n",
                "                info = client.get_collection(collection_name)\n",
                "                print(f\"‚úÖ Collection '{collection_name}' exists ({info.points_count} vectors)\")\n",
                "                need_build = False\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Warning checking collection: {e}\")\n",
                "        finally:\n",
                "            if client:\n",
                "                client.close()  # Ensure lock is released\n",
                "    \n",
                "    if need_build:\n",
                "        print(f\"üîÑ Building collection '{collection_name}' for {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            print(f\"‚úÇÔ∏è TEST MODE: Slicing to first {TEST_SUBSET_SIZE} chunks (from {len(docs)}) \")\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "            \n",
                "        build_vector_store(docs, persist_dir=QDRANT_PATH, collection_name=collection_name)\n",
                "        print(\"‚úÖ Built and saved\")\n",
                "    \n",
                "    return collection_name"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "main-loop",
            "metadata": {},
            "source": [
                "## 3. Main Processing Loop\n",
                "\n",
                "Iterate over domains, build stores, and run retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load-convs",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÇ Loading conversations...\n",
                        "Total conversations: 110\n"
                    ]
                }
            ],
            "source": [
                "# Load ALL conversations once\n",
                "print(\"üìÇ Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Total conversations: {len(all_conversations)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "process-all",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "üåç PROCESSING DOMAIN: GOVT\n",
                        "========================================\n",
                        "‚úÖ Collection 'mtrag_govt' exists (1280 vectors)\n",
                        "Found 28 conversations for govt\n",
                        "‚úÇÔ∏è TEST MODE: Processing only first 10 conversations\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/marcantoniolopez/Documenti/github/projects/llm-semeval-task8/src/retrieval.py:49: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
                        "  embedding_model = HuggingFaceEmbeddings(\n"
                    ]
                }
            ],
            "source": [
                "all_results = []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*40}\\nüåç PROCESSING DOMAIN: {domain.upper()}\\n{'='*40}\")\n",
                "    \n",
                "    # 1. Setup Vector Store\n",
                "    try:\n",
                "        collection_name = ensure_vector_store(domain)\n",
                "        if not collection_name:\n",
                "            continue\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Critical error setting up vector store for {domain}: {e}\")\n",
                "        continue\n",
                "        \n",
                "    # 2. Filter Conversations (FIXED: Substring matching)\n",
                "    domain_convs = [\n",
                "        c for c in all_conversations \n",
                "        if domain.lower() in c.get(\"domain\", \"\").lower()\n",
                "    ]\n",
                "    print(f\"Found {len(domain_convs)} conversations for {domain}\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "        \n",
                "    if TEST_MODE:\n",
                "        print(f\"‚úÇÔ∏è TEST MODE: Processing only first {TEST_QUERY_LIMIT} conversations\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    # 3. Init Retriever\n",
                "    retriever = get_retriever(\n",
                "        qdrant_path=QDRANT_PATH,\n",
                "        collection_name=collection_name,\n",
                "        top_k_retrieve=TOP_K_RETRIEVE,\n",
                "        top_k_rerank=TOP_K_RERANK\n",
                "    )\n",
                "    \n",
                "    # 4. Run Retrieval\n",
                "    print(f\"üöÄ Running retrieval for {domain}...\")\n",
                "    for conv in tqdm(domain_convs):\n",
                "        query = extract_last_query(conv.get(\"turns\", []))\n",
                "        if not query: \n",
                "            continue\n",
                "            \n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Error: {e}\")\n",
                "            docs = []\n",
                "            \n",
                "        # Format\n",
                "        contexts = []\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": meta.get(\"parent_text\") or doc.page_content\n",
                "            })\n",
                "            \n",
                "        all_results.append({\n",
                "            \"conversation_id\": conv.get(\"conversation_id\"),\n",
                "            \"task_id\": f\"{conv.get('conversation_id')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": t[\"role\"], \"text\": t[\"content\"]} for t in conv[\"turns\"]],\n",
                "            \"contexts\": contexts\n",
                "        })"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save-section",
            "metadata": {},
            "source": [
                "## 4. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nüíæ Saving {len(all_results)} total results to {OUTPUT_FILE}...\")\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "print(\"‚úÖ Done!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
