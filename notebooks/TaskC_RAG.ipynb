{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Task C: Retrieval-Augmented Generation\n",
                "\n",
                "This notebook implements **Task C** of the MTRAGEval benchmark, which evaluates the complete RAG pipeline: retrieving relevant documents and generating grounded answers.\n",
                "\n",
                "---\n",
                "\n",
                "### Objective\n",
                "\n",
                "Given a multi-turn conversation:\n",
                "1. Retrieve the most relevant documents from the corpus\n",
                "2. Generate an answer that is grounded in the retrieved context\n",
                "\n",
                "### Evaluation Criteria\n",
                "\n",
                "| Criterion | Description |\n",
                "|-----------|-------------|\n",
                "| **Faithfulness** | Answer is supported by retrieved context |\n",
                "| **Relevance** | Answer addresses the user's question |\n",
                "| **Completeness** | All key aspects are covered |\n",
                "| **Coherence** | Response is well-structured and clear |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import zipfile\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Project root detection\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Retriever settings\n",
                "TOP_K_RETRIEVE = 20\n",
                "TOP_K_RERANK = 5\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "\n",
                "# Execution mode\n",
                "TEST_MODE = True\n",
                "TEST_SUBSET_SIZE = 1000\n",
                "TEST_QUERY_LIMIT = 5\n",
                "\n",
                "# Paths\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)\n",
                "\n",
                "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
                "print(f\"Output: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages: list) -> str:\n",
                "    \"\"\"Extract the most recent user query from a conversation.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "\n",
                "def get_corpus_file(domain: str) -> str:\n",
                "    \"\"\"Get corpus file path, extracting from ZIP if necessary.\"\"\"\n",
                "    jsonl_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "    zip_path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl.zip\")\n",
                "    \n",
                "    if not os.path.exists(jsonl_path):\n",
                "        if os.path.exists(zip_path):\n",
                "            print(f\"Extracting {domain}.jsonl...\")\n",
                "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
                "                zf.extractall(CORPUS_BASE_DIR)\n",
                "        else:\n",
                "            return None\n",
                "    return jsonl_path\n",
                "\n",
                "\n",
                "def format_conversation_history(messages: list) -> str:\n",
                "    \"\"\"Format conversation history for context-aware generation.\"\"\"\n",
                "    history = []\n",
                "    for msg in messages[:-1]:\n",
                "        speaker = msg.get(\"speaker\", \"unknown\").capitalize()\n",
                "        text = msg.get(\"text\", \"\")\n",
                "        history.append(f\"{speaker}: {text}\")\n",
                "    return \"\\n\".join(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "index_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Build Vector Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_index",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for existing collection\n",
                "need_build = True\n",
                "\n",
                "if os.path.exists(QDRANT_PATH):\n",
                "    try:\n",
                "        client = get_qdrant_client(QDRANT_PATH)\n",
                "        if client.collection_exists(COLLECTION_NAME):\n",
                "            info = client.get_collection(COLLECTION_NAME)\n",
                "            print(f\"Existing collection found: {info.points_count} vectors\")\n",
                "            need_build = False\n",
                "    except Exception as e:\n",
                "        print(f\"Warning: {e}\")\n",
                "\n",
                "if need_build:\n",
                "    print(f\"Building collection '{COLLECTION_NAME}'...\")\n",
                "    all_docs = []\n",
                "    \n",
                "    for domain in DOMAINS:\n",
                "        corpus_path = get_corpus_file(domain)\n",
                "        if not corpus_path:\n",
                "            continue\n",
                "        \n",
                "        print(f\"Loading {domain}...\")\n",
                "        docs = load_and_chunk_data(corpus_path)\n",
                "        \n",
                "        for doc in docs:\n",
                "            doc.metadata[\"domain\"] = domain\n",
                "        \n",
                "        if TEST_MODE and len(docs) > TEST_SUBSET_SIZE:\n",
                "            docs = docs[:TEST_SUBSET_SIZE]\n",
                "        \n",
                "        all_docs.extend(docs)\n",
                "        print(f\"  Added {len(docs)} chunks\")\n",
                "    \n",
                "    print(f\"Total: {len(all_docs)} documents\")\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "    print(\"Index built successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "retriever_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Initialize Retriever"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_retriever",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing retriever...\")\n",
                "retriever = get_retriever(\n",
                "    qdrant_path=QDRANT_PATH,\n",
                "    collection_name=COLLECTION_NAME,\n",
                "    top_k_retrieve=TOP_K_RETRIEVE,\n",
                "    top_k_rerank=TOP_K_RERANK\n",
                ")\n",
                "print(\"Retriever ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "llm_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Initialize Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
                "\n",
                "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
                "\n",
                "print(f\"Loading model: {MODEL_ID}\")\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "        repetition_penalty=1.1,\n",
                "        return_full_text=False,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "    llm = HuggingFacePipeline(pipeline=pipe)\n",
                "    print(\"Model loaded successfully.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    from langchain.llms.fake import FakeListLLM\n",
                "    llm = FakeListLLM(responses=[\"[Dummy response]\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. RAG Prompt Template\n",
                "\n",
                "The prompt is designed for **grounded generation** with strict faithfulness to retrieved context:\n",
                "\n",
                "- **Role**: Document-grounded QA specialist\n",
                "- **Context Grounding**: Explicit instruction to use only provided documents\n",
                "- **Hallucination Prevention**: Clear guidance on handling insufficient context\n",
                "- **Citation Awareness**: Encourages traceability to source"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompt",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TASK C PROMPT: RETRIEVAL-AUGMENTED GENERATION\n",
                "# ============================================================\n",
                "\n",
                "PROMPT_TEMPLATE = \"\"\"You are a document-grounded question answering specialist. Your expertise is synthesizing information from retrieved documents to provide accurate, well-supported answers.\n",
                "\n",
                "ROLE:\n",
                "- You are a precise, factual assistant that bases answers strictly on provided documents\n",
                "- You prioritize accuracy over speculation\n",
                "- You clearly distinguish between what the documents state and what is uncertain\n",
                "\n",
                "TASK:\n",
                "Answer the user's question using ONLY the information in the provided context documents. Follow the grounding rules strictly.\n",
                "\n",
                "GROUNDING RULES:\n",
                "1. Base your answer EXCLUSIVELY on the provided context\n",
                "2. If the context contains sufficient information, synthesize a coherent, complete answer\n",
                "3. If the context is partially relevant, answer what you can and note what is missing\n",
                "4. If the context does not contain relevant information, respond: \"The provided documents do not contain sufficient information to answer this question.\"\n",
                "5. DO NOT introduce information from outside the context, even if you know it to be true\n",
                "6. Maintain the original meaning and do not over-interpret\n",
                "\n",
                "RESPONSE FORMAT:\n",
                "- Provide a direct answer to the question\n",
                "- Be concise but complete\n",
                "- Use clear, professional language\n",
                "\n",
                "---\n",
                "RETRIEVED CONTEXT:\n",
                "{context}\n",
                "---\n",
                "\n",
                "CONVERSATION HISTORY:\n",
                "{conversation_history}\n",
                "\n",
                "USER QUERY: {question}\n",
                "\n",
                "RESPONSE:\"\"\"\n",
                "\n",
                "\n",
                "def generate_answer(question: str, context: str, conversation_history: str = \"\") -> str:\n",
                "    \"\"\"Generate a grounded answer using RAG.\"\"\"\n",
                "    prompt = PROMPT_TEMPLATE.format(\n",
                "        question=question,\n",
                "        context=context if context.strip() else \"[No documents retrieved]\",\n",
                "        conversation_history=conversation_history if conversation_history else \"[No prior context]\"\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        return llm.invoke(prompt)\n",
                "    except Exception as e:\n",
                "        return f\"[Generation error: {e}]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Execute RAG Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load conversations\n",
                "print(\"Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Loaded {len(all_conversations)} conversations.\")\n",
                "\n",
                "all_results = []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Domain: {domain.upper()}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations.\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    if TEST_MODE:\n",
                "        print(f\"Test mode: limiting to {TEST_QUERY_LIMIT} queries.\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=domain):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # Retrieve documents\n",
                "        try:\n",
                "            docs = retriever.invoke(query)\n",
                "        except Exception as e:\n",
                "            print(f\"Retrieval error: {e}\")\n",
                "            docs = []\n",
                "        \n",
                "        # Format contexts\n",
                "        contexts = []\n",
                "        context_text = \"\"\n",
                "        for i, doc in enumerate(docs):\n",
                "            meta = doc.metadata\n",
                "            parent_text = meta.get(\"parent_text\") or doc.page_content\n",
                "            contexts.append({\n",
                "                \"document_id\": str(meta.get(\"doc_id\") or meta.get(\"parent_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(meta.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": parent_text\n",
                "            })\n",
                "            context_text += f\"[Document {i+1}]\\n{parent_text}\\n\\n\"\n",
                "        \n",
                "        # Format conversation history\n",
                "        history = format_conversation_history(messages)\n",
                "        \n",
                "        # Generate answer with RAG\n",
                "        answer = generate_answer(query, context_text.strip(), history)\n",
                "        \n",
                "        # Build result\n",
                "        all_results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages],\n",
                "            \"contexts\": contexts,\n",
                "            \"predictions\": [{\"text\": answer}]\n",
                "        })\n",
                "\n",
                "print(f\"\\nTotal results: {len(all_results)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Saving {len(all_results)} results to {OUTPUT_FILE}...\")\n",
                "\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "\n",
                "print(\"Saved successfully.\")\n",
                "\n",
                "# Validation\n",
                "if all_results:\n",
                "    sample = all_results[0]\n",
                "    has_contexts = \"contexts\" in sample and isinstance(sample[\"contexts\"], list)\n",
                "    has_predictions = \"predictions\" in sample and isinstance(sample[\"predictions\"], list)\n",
                "    \n",
                "    if has_contexts and has_predictions:\n",
                "        print(\"Validation: PASS - Structure correct.\")\n",
                "    else:\n",
                "        print(\"Validation: FAIL - Invalid structure.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}