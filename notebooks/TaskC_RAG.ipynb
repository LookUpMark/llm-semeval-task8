{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Task C: Retrieval-Augmented Generation\n",
                "\n",
                "This notebook implements **Task C**: retrieving relevant documents and generating answers grounded in that context.\n",
                "\n",
                "**Note:** Uses the centralized `src.generation` module for LLM and RAG prompts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Project Root\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.ingestion import load_and_chunk_data, build_vector_store\n",
                "from src.retrieval import get_retriever, get_qdrant_client\n",
                "from src.generation import create_generation_components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "COLLECTION_NAME = \"mtrag_unified\"\n",
                "TEST_MODE = True\n",
                "TEST_QUERY_LIMIT = 5\n",
                "\n",
                "CORPUS_BASE_DIR = os.path.join(PROJECT_ROOT, \"dataset/corpora/passage_level\")\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "QDRANT_PATH = os.path.join(PROJECT_ROOT, \"qdrant_db\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskC_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(QDRANT_PATH, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_index",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build Index (skip if exists)\n",
                "if not os.path.exists(QDRANT_PATH):\n",
                "    print(\"Building Index...\")\n",
                "    all_docs = []\n",
                "    for domain in DOMAINS:\n",
                "        path = os.path.join(CORPUS_BASE_DIR, f\"{domain}.jsonl\")\n",
                "        if os.path.exists(path):\n",
                "            docs = load_and_chunk_data(path)\n",
                "            for d in docs: d.metadata[\"domain\"] = domain\n",
                "            if TEST_MODE: docs = docs[:1000]\n",
                "            all_docs.extend(docs)\n",
                "    build_vector_store(all_docs, persist_dir=QDRANT_PATH, collection_name=COLLECTION_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_components",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Initializing Components...\")\n",
                "retriever = get_retriever(qdrant_path=QDRANT_PATH, collection_name=COLLECTION_NAME)\n",
                "# uses default cached model: meta-llama/Llama-3.1-8B-Instruct\n",
                "components = create_generation_components()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages):\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\": return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "with open(CONVERSATIONS_FILE) as f: all_convs = json.load(f)\n",
                "\n",
                "results = []\n",
                "for domain in DOMAINS:\n",
                "    convs = [c for c in all_convs if domain in c.get(\"domain\", \"\").lower()]\n",
                "    if TEST_MODE: convs = convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(convs, desc=domain):\n",
                "        q = extract_last_query(conv.get(\"messages\", []))\n",
                "        if not q: continue\n",
                "        \n",
                "        # 1. Retrieve\n",
                "        docs = retriever.invoke(q)\n",
                "        \n",
                "        # 2. Format Context\n",
                "        context_text = \"\"\n",
                "        contexts = []\n",
                "        for i, d in enumerate(docs):\n",
                "            txt = d.metadata.get(\"parent_text\") or d.page_content\n",
                "            context_text += f\"[Document {i+1}]\\n{txt}\\n\\n\"\n",
                "            contexts.append({\n",
                "                \"document_id\": str(d.metadata.get(\"doc_id\") or f\"{domain}_{i}\"),\n",
                "                \"score\": float(d.metadata.get(\"relevance_score\") or 0.0),\n",
                "                \"text\": txt\n",
                "            })\n",
                "        \n",
                "        # 3. Generate with RAG (using PRE-BUILT generator from src.generation)\n",
                "        try:\n",
                "            # src.generation.generator expects 'context' and 'question'\n",
                "            ans = components.generator.invoke({\"context\": context_text, \"question\": q})\n",
                "        except Exception as e:\n",
                "            ans = str(e)\n",
                "        \n",
                "        results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"contexts\": contexts,\n",
                "            \"predictions\": [{\"text\": ans}]\n",
                "        })\n",
                "\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    for r in results: f.write(json.dumps(r) + '\\n')\n",
                "print(\"Saved.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}