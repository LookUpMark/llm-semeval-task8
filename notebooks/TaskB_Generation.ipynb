{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Task B: Standalone Generation\n",
                "\n",
                "This notebook implements **Task B**: answering questions using the LLM's parametric knowledge without retrieval.\n",
                "\n",
                "**Note:** Uses the centralized `src.generation` module for LLM loading to ensure consistency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Project Root\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "from src.generation import create_generation_components\n",
                "from langchain_core.prompts import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "TEST_MODE = True\n",
                "TEST_QUERY_LIMIT = 5\n",
                "\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load centralized components\n",
                "print(\"Loading LLM...\")\n",
                "components = create_generation_components()\n",
                "\n",
                "# Custom Prompt for Task B (No RAG)\n",
                "task_b_prompt = PromptTemplate(\n",
                "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                "You are an expert assistant. Answer based on your knowledge.\n",
                "Be concise and professional.<|eot_id|>\n",
                "<|start_header_id|>user<|end_header_id|>\n",
                "{question}<|eot_id|>\n",
                "<|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
                "    input_variables=[\"question\"]\n",
                ")\n",
                "\n",
                "chain = task_b_prompt | components.llm | StrOutputParser()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages):\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\": return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "with open(CONVERSATIONS_FILE) as f: all_convs = json.load(f)\n",
                "\n",
                "results = []\n",
                "for domain in DOMAINS:\n",
                "    convs = [c for c in all_convs if domain in c.get(\"domain\", \"\").lower()]\n",
                "    if TEST_MODE: convs = convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(convs, desc=domain):\n",
                "        q = extract_last_query(conv.get(\"messages\", []))\n",
                "        if not q: continue\n",
                "        try:\n",
                "            ans = chain.invoke({\"question\": q})\n",
                "        except Exception as e:\n",
                "            ans = str(e)\n",
                "        \n",
                "        results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"predictions\": [{\"text\": ans}]\n",
                "        })\n",
                "\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    for r in results: f.write(json.dumps(r) + '\\n')\n",
                "print(\"Saved.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}