{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "task-b-header",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8 - Task B: Generation\n",
                "\n",
                "This notebook focuses on **Task B: Generation**. \n",
                "Goal: Given a question (and potentially contexts), generate an accurate answer.\n",
                "\n",
                "**Output Format**:\n",
                "The submission file must contain a `predictions` field (List of Objects), where each object has:\n",
                "- `text`: The generated answer string."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kaggle-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- KAGGLE SETUP ---\n",
                "# Uncomment and run this cell FIRST if you are running on Kaggle.\n",
                "# It clones the repo, installs dependencies, and sets the working directory.\n",
                "\n",
                "# import os\n",
                "# if not os.path.exists(\"llm-semeval-task8\"):\n",
                "#     !git clone https://github.com/LookUpMark/llm-semeval-task8.git\n",
                "\n",
                "# %cd llm-semeval-task8\n",
                "# !git checkout dev\n",
                "# !pip install -c scripts/evaluation/constraints.txt -r scripts/evaluation/requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import sys\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Locate Project Root (Robust for Local vs Kaggle)\n",
                "if os.path.exists(\"src\"):\n",
                "    # We are in the root (e.g. Kaggle after %cd)\n",
                "    project_root = os.getcwd()\n",
                "else:\n",
                "    # We are likely in 'notebooks/' (Local)\n",
                "    project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
                "\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "try:\n",
                "    from src.graph import app\n",
                "except ImportError:\n",
                "    print(\"ERROR: Check imports.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CONFIGURATION ---\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "TASK_TYPE = \"TaskB\"\n",
                "# Adjust path based on execution environment\n",
                "base_path = \".\" if os.path.exists(\"dataset\") else \"..\"\n",
                "INPUT_FILE = os.path.join(base_path, \"dataset/human/generation_tasks/reference.jsonl\")\n",
                "OUTPUT_DIR = os.path.join(base_path, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_{TASK_TYPE}_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Target: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(INPUT_FILE) as f:\n",
                "    test_data = [json.loads(line) for line in f if line.strip()]\n",
                "print(f\"Loaded {len(test_data)} items.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run-gen",
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "\n",
                "print(\"Running Generation Task...\")\n",
                "for item in tqdm(test_data):\n",
                "    question = item.get(\"question\")\n",
                "    \n",
                "    try:\n",
                "        # Invoke Graph\n",
                "        # NOTE: If your graph supports 'documents' as input, you could pass \n",
                "        # item.get('contexts') here to do purely generation.\n",
                "        # Otherwise, this runs Retrieval+Generation (which is also valid).\n",
                "        resp = app.invoke({\"question\": question})\n",
                "        gen_text = resp.get(\"generation\", \"\")\n",
                "        if not gen_text: gen_text = \"No Answer\"\n",
                "    except Exception as e:\n",
                "        print(f\"Error on '{question}': {e}\")\n",
                "        gen_text = \"Error\"\n",
                "    \n",
                "    output_item = item.copy()\n",
                "    # Task B requires 'predictions' list\n",
                "    output_item[\"predictions\"] = [{\"text\": gen_text}]\n",
                "    \n",
                "    results.append(output_item)\n",
                "\n",
                "print(f\"Done. Generated {len(results)} answers.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for x in results:\n",
                "        json.dump(x, f)\n",
                "        f.write('\\n')\n",
                "print(f\"Saved to {OUTPUT_FILE}\")\n",
                "\n",
                "# Validation\n",
                "sample = results[0]\n",
                "if \"predictions\" in sample and isinstance(sample[\"predictions\"], list) and \"text\" in sample[\"predictions\"][0]:\n",
                "    print(\"\\033[92mVALIDATION PASS: Structure correct.\\033[0m\")\n",
                "else:\n",
                "    print(\"\\033[91mVALIDATION FAIL: 'predictions' formatting error.\\033[0m\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}