{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# SemEval 2026 Task 8: Multi-Turn RAG Evaluation\n",
                "\n",
                "## Task B: Standalone Generation\n",
                "\n",
                "This notebook implements **Task B** of the MTRAGEval benchmark, which evaluates the quality of answer generation using only the language model's parametric knowledge (no retrieval).\n",
                "\n",
                "---\n",
                "\n",
                "### Objective\n",
                "\n",
                "Given a multi-turn conversation, generate an accurate and informative answer using only the LLM's internal knowledge, without access to external documents.\n",
                "\n",
                "### Evaluation Focus\n",
                "\n",
                "- **Factual Accuracy**: Correctness of generated information\n",
                "- **Relevance**: How well the answer addresses the query\n",
                "- **Coherence**: Logical structure and clarity\n",
                "- **Completeness**: Coverage of key aspects"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Project root detection\n",
                "if os.path.exists(\"src\"):\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "else:\n",
                "    PROJECT_ROOT = os.path.abspath(\"..\")\n",
                "\n",
                "if PROJECT_ROOT not in sys.path:\n",
                "    sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "print(f\"Project Root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================\n",
                "\n",
                "TEAM_NAME = \"Gbgers\"\n",
                "DOMAINS = [\"govt\", \"clapnq\", \"fiqa\", \"cloud\"]\n",
                "\n",
                "# Execution mode\n",
                "TEST_MODE = True\n",
                "TEST_QUERY_LIMIT = 5\n",
                "\n",
                "# Paths\n",
                "CONVERSATIONS_FILE = os.path.join(PROJECT_ROOT, \"dataset/human/conversations/conversations.json\")\n",
                "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"data/submissions\")\n",
                "OUTPUT_FILE = os.path.join(OUTPUT_DIR, f\"submission_TaskB_{TEAM_NAME}.jsonl\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Mode: {'TEST' if TEST_MODE else 'FULL'}\")\n",
                "print(f\"Output: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helpers",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_last_query(messages: list) -> str:\n",
                "    \"\"\"Extract the most recent user query from a conversation.\"\"\"\n",
                "    for msg in reversed(messages):\n",
                "        if msg.get(\"speaker\") == \"user\":\n",
                "            return msg.get(\"text\", \"\")\n",
                "    return \"\"\n",
                "\n",
                "\n",
                "def format_conversation_history(messages: list) -> str:\n",
                "    \"\"\"Format conversation history for context-aware generation.\"\"\"\n",
                "    history = []\n",
                "    for msg in messages[:-1]:  # Exclude last message (the query)\n",
                "        speaker = msg.get(\"speaker\", \"unknown\").capitalize()\n",
                "        text = msg.get(\"text\", \"\")\n",
                "        history.append(f\"{speaker}: {text}\")\n",
                "    return \"\\n\".join(history)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "llm_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Initialize Language Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "init_llm",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from langchain_huggingface import HuggingFacePipeline\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
                "\n",
                "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
                "\n",
                "print(f\"Loading model: {MODEL_ID}\")\n",
                "\n",
                "# 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "try:\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_ID,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "\n",
                "    pipe = pipeline(\n",
                "        \"text-generation\",\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        max_new_tokens=256,\n",
                "        temperature=0.1,\n",
                "        do_sample=True,\n",
                "        repetition_penalty=1.1,\n",
                "        return_full_text=False,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "    llm = HuggingFacePipeline(pipeline=pipe)\n",
                "    print(\"Model loaded successfully.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error loading model: {e}\")\n",
                "    from langchain.llms.fake import FakeListLLM\n",
                "    llm = FakeListLLM(responses=[\"[Dummy response]\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "prompt_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Prompt Template\n",
                "\n",
                "The prompt is designed using established prompt engineering principles:\n",
                "\n",
                "- **Role Definition**: Expert assistant with domain expertise\n",
                "- **Task Specification**: Clear instruction on response requirements\n",
                "- **Behavioral Guidelines**: Constraints on response quality\n",
                "- **Conversation Awareness**: Multi-turn context handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prompt",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TASK B PROMPT: STANDALONE GENERATION\n",
                "# ============================================================\n",
                "\n",
                "PROMPT_TEMPLATE = \"\"\"You are an expert assistant with comprehensive knowledge spanning government policy, technology, finance, and general knowledge domains.\n",
                "\n",
                "ROLE:\n",
                "- You are a knowledgeable, helpful, and precise assistant\n",
                "- You provide accurate information based on your training knowledge\n",
                "- You communicate clearly and professionally\n",
                "\n",
                "TASK:\n",
                "Answer the user's question based solely on your internal knowledge. Do not reference or request external documents.\n",
                "\n",
                "GUIDELINES:\n",
                "1. Provide a direct, focused answer to the question\n",
                "2. Be factually accurate - if uncertain, acknowledge limitations\n",
                "3. Structure your response logically\n",
                "4. Be concise while ensuring completeness\n",
                "5. Use clear, professional language\n",
                "\n",
                "---\n",
                "CONVERSATION CONTEXT:\n",
                "{conversation_history}\n",
                "---\n",
                "\n",
                "USER QUERY: {question}\n",
                "\n",
                "RESPONSE:\"\"\"\n",
                "\n",
                "\n",
                "def generate_answer(question: str, conversation_history: str = \"\") -> str:\n",
                "    \"\"\"Generate an answer using the LLM.\"\"\"\n",
                "    prompt = PROMPT_TEMPLATE.format(\n",
                "        question=question,\n",
                "        conversation_history=conversation_history if conversation_history else \"[No prior context]\"\n",
                "    )\n",
                "    \n",
                "    try:\n",
                "        return llm.invoke(prompt)\n",
                "    except Exception as e:\n",
                "        return f\"[Generation error: {e}]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Execute Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load conversations\n",
                "print(\"Loading conversations...\")\n",
                "with open(CONVERSATIONS_FILE, 'r') as f:\n",
                "    all_conversations = json.load(f)\n",
                "print(f\"Loaded {len(all_conversations)} conversations.\")\n",
                "\n",
                "all_results = []\n",
                "\n",
                "for domain in DOMAINS:\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Domain: {domain.upper()}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    # Filter by domain\n",
                "    domain_convs = [c for c in all_conversations if domain.lower() in c.get(\"domain\", \"\").lower()]\n",
                "    print(f\"Found {len(domain_convs)} conversations.\")\n",
                "    \n",
                "    if not domain_convs:\n",
                "        continue\n",
                "    \n",
                "    if TEST_MODE:\n",
                "        print(f\"Test mode: limiting to {TEST_QUERY_LIMIT} queries.\")\n",
                "        domain_convs = domain_convs[:TEST_QUERY_LIMIT]\n",
                "    \n",
                "    for conv in tqdm(domain_convs, desc=domain):\n",
                "        messages = conv.get(\"messages\", [])\n",
                "        query = extract_last_query(messages)\n",
                "        \n",
                "        if not query:\n",
                "            continue\n",
                "        \n",
                "        # Format conversation history\n",
                "        history = format_conversation_history(messages)\n",
                "        \n",
                "        # Generate answer\n",
                "        answer = generate_answer(query, history)\n",
                "        \n",
                "        # Build result\n",
                "        all_results.append({\n",
                "            \"conversation_id\": conv.get(\"author\"),\n",
                "            \"task_id\": f\"{conv.get('author')}::1\",\n",
                "            \"Collection\": f\"mt-rag-{domain}\",\n",
                "            \"input\": [{\"speaker\": m[\"speaker\"], \"text\": m[\"text\"]} for m in messages],\n",
                "            \"predictions\": [{\"text\": answer}]\n",
                "        })\n",
                "\n",
                "print(f\"\\nTotal results: {len(all_results)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_section",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Saving {len(all_results)} results to {OUTPUT_FILE}...\")\n",
                "\n",
                "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
                "    for item in all_results:\n",
                "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
                "\n",
                "print(\"Saved successfully.\")\n",
                "\n",
                "# Validation\n",
                "if all_results:\n",
                "    sample = all_results[0]\n",
                "    if \"predictions\" in sample and isinstance(sample[\"predictions\"], list):\n",
                "        print(\"Validation: PASS - Structure correct.\")\n",
                "    else:\n",
                "        print(\"Validation: FAIL - Invalid structure.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}