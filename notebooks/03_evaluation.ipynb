{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MTRAGEval - Evaluation\n",
                "\n",
                "This notebook evaluates the Self-CRAG pipeline using RAGAS metrics.\n",
                "\n",
                "## Metrics:\n",
                "- **Faithfulness**: How well is the answer grounded in context?\n",
                "- **Answer Relevancy**: How relevant is the answer to the question?\n",
                "- **Context Precision**: How precise is the retrieved context?\n",
                "\n",
                "Uses Llama 3.1 as the judge model (instead of OpenAI)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q ragas==0.1.4 datasets==2.18.0\n",
                "!pip install -q langchain==0.1.10 langchain-community==0.0.25 langchain-huggingface==0.0.3 langgraph==0.0.26\n",
                "!pip install -q transformers accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '../')\n",
                "\n",
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Test Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample test dataset\n",
                "# Replace with actual mtRAG test data\n",
                "test_dataset = [\n",
                "    {\n",
                "        \"question\": \"Who is the CEO of Apple?\",\n",
                "        \"ground_truth\": \"Tim Cook\"\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"What is the capital of France?\",\n",
                "        \"ground_truth\": \"Paris\"\n",
                "    }\n",
                "]\n",
                "\n",
                "print(f\"Test dataset size: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize Pipeline and Run Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "from datasets import Dataset\n",
                "\n",
                "def run_pipeline_on_dataset(app, test_data):\n",
                "    \"\"\"\n",
                "    Run the pipeline on test dataset and collect results.\n",
                "    \n",
                "    Returns dict with:\n",
                "    - questions: List of questions\n",
                "    - answers: List of generated answers\n",
                "    - contexts: List of retrieved contexts\n",
                "    - ground_truths: List of expected answers\n",
                "    \n",
                "    TODO: Implement pipeline execution\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement pipeline execution on dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run inference\n",
                "# results = run_pipeline_on_dataset(app, test_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. RAGAS Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ragas import evaluate\n",
                "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
                "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
                "\n",
                "RAGAS_EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
                "\n",
                "def run_ragas_evaluation(results_data, llm):\n",
                "    \"\"\"\n",
                "    Run RAGAS evaluation with local Llama judge.\n",
                "    \n",
                "    Args:\n",
                "        results_data: Dict with questions, answers, contexts, ground_truths\n",
                "        llm: HuggingFacePipeline for evaluation\n",
                "        \n",
                "    Returns:\n",
                "        RAGAS scores dict\n",
                "        \n",
                "    TODO: Implement RAGAS evaluation\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement RAGAS evaluation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation\n",
                "# scores = run_ragas_evaluation(results, llm)\n",
                "# print(\"\\n=== EVALUATION RESULTS ===\")\n",
                "# print(scores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyze Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_idk_accuracy(results):\n",
                "    \"\"\"\n",
                "    Calculate I_DONT_KNOW (Refusal) accuracy.\n",
                "    \n",
                "    Key metric for mtRAG: System should refuse when\n",
                "    documents don't support the answer.\n",
                "    \n",
                "    TODO: Implement IDK analysis\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement IDK accuracy calculation\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze IDK accuracy\n",
                "# idk_stats = analyze_idk_accuracy(results)\n",
                "# print(f\"IDK Accuracy: {idk_stats['accuracy']:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "\n",
                "def export_results(results, scores, output_path):\n",
                "    \"\"\"\n",
                "    Export evaluation results to JSON/CSV.\n",
                "    \n",
                "    TODO: Implement results export\n",
                "    \"\"\"\n",
                "    raise NotImplementedError(\"Implement results export\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export results\n",
                "# export_results(results, scores, \"../eval/results.json\")\n",
                "# print(\"Results exported!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}